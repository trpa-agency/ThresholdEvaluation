{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "from arcgis import GIS\n",
    "from arcgis.features import FeatureLayer\n",
    "from arcgis.features import GeoAccessor\n",
    "import os\n",
    "import numpy as np\n",
    "import arcpy\n",
    "from soil_conservation import *\n",
    "workspace =r'F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\n",
    "draftworkspace =r'C:\\Users\\snewsome\\Documents\\GitHub\\ThresholdEvaluation\\2023\\WaterQuality' \n",
    "# make sql database connection with pyodbc\n",
    "#conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER=sql12;DATABASE=sde_tabular;UID=sde;PWD=staff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soil Conservation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfImpChg.GISAcre.sum()\n",
    "# group by Surface and sum\n",
    "df = dfImpChg.groupby('Surface')[\"GISAcre\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get land capability data\n",
    "dfImpChg, dfImp2019 = get_soil_conservation_data_web()\n",
    "\n",
    "df = dfImpChg.astype({\"Land_Capab\": str})\n",
    "df = df.rename(columns={'Land_Capab':'Land Capability', 'GISAcre':'Total Acres', 'OWNERSHIP_': 'Ownership'})\n",
    "df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "df = df[df['Land Capability'].notna()]\n",
    "df.set_index('Land Capability')\n",
    "dfLCType = df.groupby(\"Land Capability\")[\"Total Acres\"].sum().reset_index()\n",
    "\n",
    "df = dfLCType.replace(to_replace='None', value=np.nan).dropna()\n",
    "\n",
    "# 2019 analysis results\n",
    "df = dfImp2019.astype({\"Land_Capab\": str})\n",
    "df = df.rename(columns={'Land_Capab':'Land Capability', 'GISAcre':'Acre'})\n",
    "df.set_index('Land Capability')\n",
    "\n",
    "# trails were drawn to wide, reduce acreage by 50%\n",
    "df.loc[(df['Surface']=='Soft')&(df['Feature']=='Trail'), 'Acre'] = df.Acre * 0.5 \n",
    "\n",
    "# pivot land capbility by acres of surface type\n",
    "pivotSoilImp = pd.pivot_table(df,index=['Land Capability'],\n",
    "                            columns='Surface',\n",
    "                            values=['Acre'], \n",
    "                            aggfunc=np.sum,fill_value=0)\n",
    "\n",
    "flattened = pd.DataFrame(pivotSoilImp.to_records())\n",
    "\n",
    "df = flattened.rename(columns={\"('Acre', 'Hard')\":'Acres of Hard Surface 2019',\n",
    "                            \"('Acre', 'Soft')\":'Acres of Soft Surface 2019'})\n",
    "df\n",
    "# # replace all spaces ond blanks with NaN\n",
    "# df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "# df = df[df['Land Capability'].notna()]\n",
    "\n",
    "# # calculate acres of coverage\n",
    "# df[\"Acres of Coverage 2019\"]= df[\"Acres of Hard Surface 2019\"]+df[\"Acres of Soft Surface 2019\"]\n",
    "\n",
    "# # merge grouped land capability data frame with impervious pivot data frame\n",
    "# dfMerge = pd.merge(df, dfLCType, on='Land Capability')\n",
    "\n",
    "# # rename field\n",
    "# df = dfMerge.rename(columns={'Acre':'Total Acres'})\n",
    "\n",
    "# # calculate perent coverage\n",
    "# df['Percent Hard 2019'] = (df['Acres of Hard Surface 2019']/df['Total Acres'])*100\n",
    "# df['Percent Soft 2019'] = (df['Acres of Soft Surface 2019']/df['Total Acres'])*100\n",
    "# df['Percent Impervious 2019'] = ((df['Acres of Hard Surface 2019']+df['Acres of Soft Surface 2019'])/df['Total Acres'])*100\n",
    "\n",
    "# # record percent allowed field\n",
    "# df['Threshold Value'] = \"0%\"\n",
    "# df.loc[df['Land Capability'].isin(['1A','1B','1C','2']), 'Threshold Value'] = \"1%\"\n",
    "# df.loc[df['Land Capability'].isin(['3']), 'Threshold Value'] = \"5%\"\n",
    "# df.loc[df['Land Capability'].isin(['4']), 'Threshold Value'] = \"20%\"\n",
    "# df.loc[df['Land Capability'].isin(['5']), 'Threshold Value'] = \"25%\"\n",
    "# df.loc[df['Land Capability'].isin(['6','7']), 'Threshold Value'] = \"30%\"\n",
    "\n",
    "# # determine acres of coverage allowed per land capability\n",
    "# df['Threshold Acres'] = 0\n",
    "# df.loc[df['Land Capability'].isin(['1A','1B','1C','2']), 'Threshold Acres'] = df['Total Acres']*0.01\n",
    "# df.loc[df['Land Capability'].isin(['3']), 'Threshold Acres'] = df['Total Acres']*0.05\n",
    "# df.loc[df['Land Capability'].isin(['4']), 'Threshold Acres'] = df['Total Acres']*0.2\n",
    "# df.loc[df['Land Capability'].isin(['5']), 'Threshold Acres'] = df['Total Acres']*0.25\n",
    "# df.loc[df['Land Capability'].isin(['6','7']), 'Threshold Acres'] = df['Total Acres']*0.3\n",
    "\n",
    "# df2019 = df.drop(columns=['Threshold Acres', 'Threshold Value', 'Total Acres'])\n",
    "\n",
    "# df = df[['Land Capability',\n",
    "#         'Acres of Hard Surface 2019',\n",
    "#         'Acres of Soft Surface 2019',\n",
    "#         'Acres of Coverage 2019',\n",
    "#         'Percent Hard 2019',\n",
    "#         'Percent Soft 2019',\n",
    "#         'Percent Impervious 2019',\n",
    "#         'Total Acres',\n",
    "#         'Threshold Value',\n",
    "#         'Threshold Acres']]\n",
    "\n",
    "# df.dropna(subset=['Land Capability'], inplace=True)\n",
    "\n",
    "# df['Land Capability']= pd.Categorical(df['Land Capability'], ['1A', '1B', '1C', '2', '3', '4', '5', '6', '7'])\n",
    "\n",
    "# df.sort_values(by=\"Land Capability\")\n",
    "# df.set_index('Land Capability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNewImp = pd.read_csv(r\"data\\raw_data\\FinalCoverageChanges_2020-2023.csv\")\n",
    "\n",
    "# fill any NaN values with 0\n",
    "dfNewImp.fillna(0, inplace=True)\n",
    "# drop parcle and jurisdiction columns\n",
    "dfNewImp.drop(['Parcel', 'Jurisdiction'], axis=1, inplace=True)\n",
    "# group by\n",
    "df = dfNewImp.groupby(['Bailey1a', 'Bailey1b', 'Bailey1c', 'Bailey2', 'Bailey3', 'Bailey4', 'Bailey5', 'Bailey6', 'Bailey7']).sum()\n",
    "df = df.reset_index()\n",
    "# drop total column\n",
    "# df.drop('Total', axis=1, inplace=True)\n",
    "# stack the dataframe\n",
    "df = df.stack().reset_index()\n",
    "# # rename columns\n",
    "df.rename(columns={'level_1':'LandCapability', 0:'SqFt'}, inplace=True)\n",
    "# # drop columns\n",
    "df.drop(['level_0'], axis=1, inplace=True)\n",
    "# pivot the dataframe\n",
    "pivot = pd.pivot_table(df,index=['LandCapability'],\n",
    "                              values='SqFt', aggfunc=np.sum)\n",
    "# flatten pivot\n",
    "flattened = pd.DataFrame(pivot.to_records())\n",
    "# create acres column\n",
    "flattened['Acres'] = flattened['SqFt']/43560\n",
    "# to csv\n",
    "flattened.to_csv(r\"data/processed_data/LandCapability_Acres.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "#import pandas as pd\n",
    "#import os\n",
    "#from arcgis import GIS\n",
    "#from arcgis.features import FeatureLayer\n",
    "# Import geopandas\n",
    "#import geopandas as gpd\n",
    "#from shapely.geometry import Point\n",
    "#import arcpy\n",
    "#from arcgis.features import GeoAccessor\n",
    "\n",
    "\n",
    "#workspace =r'C:\\Users\\snewsome\\Documents\\ArcGIS\\Projects\\Data Management 2023\\Scratch.gdb'\n",
    "\n",
    "## Update SDE.Monitoring.SDE.Plan_Area_Noise with new data 2020-2023\n",
    "#excel_file_path = r\"F:\\Research and Analysis\\Noise\\Monitoring\\CNEL - Plan Areas\\ALL_CNEL_PLANAREAS_RESULTS_(1991-2023).xlsx\"\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "#Noise23_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Keep only columns that you want\n",
    "#columns_to_keep = ['CNEL LAND USE', '1987 PAS#', 'MONITORING SITE LATITUDE', 'MONITORING SITE LONGITUDE', 'PAS_NAME', 'LAND_USE', '2023 CNEL Maximum Day', '2022 CNEL Maximum Day', '2021 CNEL Maximum Day', '2020 CNEL Maximum Day']\n",
    "#Noise23_df = Noise23_df[columns_to_keep]\n",
    "\n",
    "#Noise23_df.columns = Noise23_df.columns.str.strip()\n",
    "\n",
    "# PASNAME\n",
    "#Noise23_df['PAS_NAME'] = Noise23_df['PAS_NAME'].astype(str)\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "#meltcolumns_to_keep = ['CNEL LAND USE', '1987 PAS#', 'MONITORING SITE LATITUDE', 'MONITORING SITE LONGITUDE', 'LAND_USE']\n",
    "#id_columns = ['PAS_NAME']\n",
    "\n",
    "# Specify the columns to melt (exclude the identifier column)\n",
    "#columns_to_melt = ['2023 CNEL Maximum Day', '2022 CNEL Maximum Day', '2021 CNEL Maximum Day', '2020 CNEL Maximum Day']\n",
    "\n",
    "#id_vars = id_columns + meltcolumns_to_keep\n",
    "\n",
    "# Use pd.melt to reshape the DataFrame\n",
    "#Noise23_df = pd.melt(Noise23_df, id_vars=id_vars, value_vars=columns_to_melt, var_name='Year', value_name='CNEL_Maximum_Day_Value')\n",
    "\n",
    "#print(Noise23_df.head())\n",
    "#print(Noise23_df.columns)\n",
    "\n",
    "# Extract only the year from the 'YEAR_OF_COUNT' column and drop \"CNEL Maximum Day\" from each value\n",
    "#Noise23_df['Year'] = Noise23_df['Year'].str.replace(' CNEL Maximum Day', '')\n",
    "\n",
    "\n",
    "\n",
    "# Add 'Category' column\n",
    "#Noise23_df['Category'] = 'CNEL Maximum Day'\n",
    "\n",
    "# Drop rows where 'CNEL_Maximum_Day_Value' is NaN\n",
    "#Noise23_df = Noise23_df.dropna(subset=['CNEL_Maximum_Day_Value'])\n",
    "\n",
    "# Use lookup_dict to fill in LTINFO# Create a lookup dictionary to fill in LTINFO and ID\n",
    "#gis = GIS()\n",
    "\n",
    "\n",
    "#def get_fs_data(service_url):\n",
    " #   feature_layer = FeatureLayer(service_url)\n",
    "  #  query_result = feature_layer.query()\n",
    "   # feature_list = query_result.features\n",
    "    #all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    #return all_data\n",
    "\n",
    "\n",
    "# REST SERVICE data for lookup dictionary\n",
    "#service_url = 'https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/6'\n",
    "\n",
    "#dfplannoisesde = get_fs_data(service_url)\n",
    "\n",
    "#dfplannoisesde.info()\n",
    "#columnstokeep = ['SITE_NAME', 'ID', 'LTINFO']\n",
    "#dfplannoisesde = dfplannoisesde.loc[:, columnstokeep]\n",
    "\n",
    "# Drop duplicates based on 'SITE_NAME' and keep the first occurrence\n",
    "#unique_values = dfplannoisesde.drop_duplicates(subset='SITE_NAME', keep='first')\n",
    "\n",
    "# Select specific columns for lookup\n",
    "#selected_columns = ['SITE_NAME', 'ID', 'LTINFO']\n",
    "\n",
    "# Convert selected columns to dictionary\n",
    "#lookup_dict = unique_values[selected_columns].set_index('SITE_NAME').to_dict(orient='index')\n",
    "\n",
    "# Display the dictionary\n",
    "#print(lookup_dict)\n",
    "\n",
    "# Create new columns 'ID' and 'LTINFO' based on 'PAS_NAME' and look up dictionary\n",
    "#Noise23_df['ID'] = Noise23_df['PAS_NAME'].apply(lambda x: lookup_dict.get(x, {}).get('ID'))\n",
    "#Noise23_df['LTINFO'] = Noise23_df['PAS_NAME'].apply(lambda x: lookup_dict.get(x, {}).get('LTINFO'))\n",
    "\n",
    "# Specify the columns to keep in Final_sdf\n",
    "#finalcolumns_to_keep = ['PAS_NAME', 'Year', 'CNEL_Maximum_Day_Value', 'Category','ID', 'MONITORING SITE LATITUDE', 'MONITORING SITE LONGITUDE', 'LAND_USE', 'CNEL LAND USE', 'LTINFO', '1987 PAS#']\n",
    "\n",
    "# Create a new DataFrame Final_df with the selected columns\n",
    "#Final_sdf = Noise23_df[finalcolumns_to_keep]\n",
    "\n",
    "# Create a GeoDataFrame by converting 'LATITUDE' and 'LONGITUDE' to Point geometry\n",
    "#geometry = [Point(xy) for xy in zip(Final_sdf['MONITORING SITE LONGITUDE'], Final_sdf['MONITORING SITE LATITUDE'])]\n",
    "#Final_sdf = gpd.GeoDataFrame(Final_sdf, geometry=geometry)\n",
    "\n",
    "# Field mapping so that columns match\n",
    "#field_mapping = {\n",
    " #   '1987 PAS#': 'PAS_1987',\n",
    "  #  'ID': 'ID',\n",
    "   # 'MONITORING SITE LATITUDE': 'LATITUDE',\n",
    "    #'MONITORING SITE LONGITUDE': 'LONGITUDE',\n",
    "    #'PAS_NAME': 'SITE_NAME',\n",
    "    #'LAND_USE': 'LAND_USE',\n",
    "    #'CNEL LAND USE': 'CNEL_LAND_USE',\n",
    "    #'Category': 'CATEGORY',\n",
    "    #'Year': 'YEAR_OF_COUNT',\n",
    "    #'CNEL_Maximum_Day_Value': 'COUNT_VALUE',\n",
    "    #'LTINFO': 'LTINFO'\n",
    "#}\n",
    "\n",
    "# Create a new DataFrame Final_df with columns based on the field mapping\n",
    "#Final_sdf = Final_sdf.rename(columns=field_mapping)\n",
    "\n",
    "# Convert GeoPandas DataFrame to ArcGIS Spatially Enabled DataFrame\n",
    "#sedf = GeoAccessor.from_geodataframe(Final_sdf)\n",
    "\n",
    "# Display the updated Final_df\n",
    "#print(list(Final_sdf.columns))\n",
    "\n",
    "# Save Final_df to a CSV file\n",
    "#Final_sdf.to_csv(r'F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Drafts\\NoiseThreshold23.csv', index=False)\n",
    "\n",
    "# Convert the SEDF to a feature class\n",
    "#sedf.spatial.to_featureclass(location=os.path.join(workspace, 'PlanAreaNoise23staging'), \n",
    " #                                                           sanitize_columns=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNEL Average Update SDE.Tabular SDE.Thresholdevaluation_PlanAreaNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update SDE.Tabular.sde.thresholdevaluation_PlanAreaNoise---Average CNEL over timeframe\n",
    "#updated to make a feature class instead of a csv\n",
    "# Setup\n",
    "import pandas as pd\n",
    "from arcgis import GIS\n",
    "from arcgis.features import FeatureLayer\n",
    "from arcgis.features import GeoAccessor\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "workspace =r'F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\n",
    "\n",
    "# Update SDE.tabular.SDE.threshhold_PlanAreaNoise with new data 2020-2023\n",
    "excel_file_path = r\"F:\\Research and Analysis\\Noise\\Monitoring\\CNEL - Plan Areas\\ALL_CNEL_PLANAREAS_RESULTS_(1991-2023).xlsx\"\n",
    "\n",
    "#update this Excel \n",
    "Excel_Final = r\"F:\\Research and Analysis\\Threshold reporting\\ThresholdData\\Noise\\ThresholdData_PlanAreaNoise.xlsx\"\n",
    "# Read the Excel file into a DataFrame\n",
    "Noise23_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Keep only columns that you want\n",
    "columns_to_keep = ['PAS_NAME', 'CNEL LAND USE', '2023 CNEL Average', '2022 CNEL Average', '2021 CNEL Average', '2020 CNEL Average', 'CNEL limit']\n",
    "Noise23_df = Noise23_df[columns_to_keep]\n",
    "\n",
    "Noise23_df.columns = Noise23_df.columns.str.strip()\n",
    "\n",
    "\n",
    "\n",
    "# melted columns to keep\n",
    "meltcolumns_to_keep = ['CNEL LAND USE']\n",
    "id_columns = ['PAS_NAME']\n",
    "\n",
    "# Specify the columns to melt (exclude the identifier column)\n",
    "columns_to_melt = ['2023 CNEL Average', '2022 CNEL Average', '2021 CNEL Average', '2020 CNEL Average']\n",
    "\n",
    "id_vars = id_columns + meltcolumns_to_keep\n",
    "\n",
    "# Use pd.melt to reshape the DataFrame\n",
    "Noise23_df = pd.melt(Noise23_df, id_vars=id_vars, value_vars=columns_to_melt, var_name='Year', value_name='CNEL_Average')\n",
    "\n",
    "# Extract only the year from the 'YEAR_OF_COUNT' column and drop \"CNEL Maximum Day\" from each value\n",
    "#Noise23_df['Year'] = Noise23_df['Year'].str.replace(' CNEL Average', '')\n",
    "# Extract only the year from the 'Year' column and drop \"CNEL Average\" from each value\n",
    "Noise23_df['Year'] = Noise23_df['Year'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# Drop rows where 'CNEL_Average' is NaN\n",
    "Noise23_df = Noise23_df.dropna(subset=['CNEL_Average'])\n",
    "\n",
    "print(Noise23_df.head())\n",
    "print(Noise23_df.columns)\n",
    "\n",
    "\n",
    "# Calculate the average for each category\n",
    "Value = Noise23_df.groupby('CNEL LAND USE')['CNEL_Average'].agg(np.mean)\n",
    "\n",
    "# Create new DataFrame with category averages\n",
    "Noiseavg_df = pd.DataFrame(Value).reset_index()\n",
    "Noiseavg_df.columns = ['CNEL LAND USE', 'Value']\n",
    "\n",
    "# Create new rows with the calculated averages and the desired year range\n",
    "year_range = '2020-2023'  # Define the desired year range\n",
    "\n",
    "Noiseavg_df['Year'] = '2020-2023'\n",
    "Noiseavg_df['Description'] = 'Average CNEL of all sites during timeframe'\n",
    "\n",
    "#Defining Threshold Value by category\n",
    "def categorize_value(row):\n",
    "    if row['CNEL LAND USE'] == 'COMMERCIAL':\n",
    "        return '60'\n",
    "    elif row['CNEL LAND USE'] == 'HIGH DENSITY RESIDENTIAL':\n",
    "        return '55'\n",
    "    elif row['CNEL LAND USE'] == 'HOTEL/MOTEL':\n",
    "        return '60'\n",
    "    elif row['CNEL LAND USE'] == 'INDUSTRIAL':\n",
    "        return '65'\n",
    "    elif row['CNEL LAND USE'] == 'LOW DENSITY RESIDENTIAL':\n",
    "        return '50'\n",
    "    elif row['CNEL LAND USE'] == 'URBAN OUTDOOR RECREATION':\n",
    "        return '55'\n",
    "    elif row['CNEL LAND USE'] == 'RURAL OUTDOOR RECREATION':\n",
    "        return '50'\n",
    "    elif row['CNEL LAND USE'] == 'WILDERNESS/ROADLESS':\n",
    "        return '45'\n",
    "\n",
    "    else:\n",
    "        return '45'\n",
    "\n",
    "# Apply the function to each row and assign the result to a new column 'Category_Type'\n",
    "Noiseavg_df['Threshold_Value'] = Noiseavg_df.apply(categorize_value, axis=1)\n",
    "\n",
    "#Defining Threshold Value by category\n",
    "def categorize_value(row):\n",
    "    if row['CNEL LAND USE'] == 'COMMERCIAL':\n",
    "        return 'Commercial Areas'\n",
    "    elif row['CNEL LAND USE'] == 'HIGH DENSITY RESIDENTIAL':\n",
    "        return 'High Density Residential'\n",
    "    elif row['CNEL LAND USE'] == 'HOTEL/MOTEL':\n",
    "        return 'Hotel / Motel Areas'\n",
    "    elif row['CNEL LAND USE'] == 'INDUSTRIAL':\n",
    "        return 'Industrial Areas'\n",
    "    elif row['CNEL LAND USE'] == 'LOW DENSITY RESIDENTIAL':\n",
    "        return 'Low Density Residential'\n",
    "    elif row['CNEL LAND USE'] == 'URBAN OUTDOOR RECREATION':\n",
    "        return 'Urban Outdoor Recreation Areas'\n",
    "    elif row['CNEL LAND USE'] == 'RURAL OUTDOOR RECREATION':\n",
    "        return 'Rural Outdoor Recreation Areas'\n",
    "    elif row['CNEL LAND USE'] == 'WILDERNESS/ROADLESS':\n",
    "        return 'Wilderness and Roadless'\n",
    "\n",
    "    else:\n",
    "        return 'Critical Wildlife Habitat'\n",
    "\n",
    "# Apply the function to each row and assign the result to a new column 'Category_Type'\n",
    "Noiseavg_df['Category'] = Noiseavg_df.apply(categorize_value, axis=1)\n",
    "\n",
    "print(Noiseavg_df)\n",
    "\n",
    "\n",
    "# Specify the columns to keep in Final_sdf\n",
    "finalcolumns_to_keep = [ 'Year', 'Threshold_Value','Description', 'Category', 'Value']\n",
    "\n",
    "# Create a new DataFrame Final_df with the selected columns\n",
    "df = Noiseavg_df[finalcolumns_to_keep]\n",
    "\n",
    "\n",
    "\n",
    "# Field mapping so that columns match\n",
    "field_mapping = {\n",
    "    'Threshold_Value': 'Threshold_Value',\n",
    "    'Category': 'Category',\n",
    "    'Description': 'Description',\n",
    "    'Year': 'Year',\n",
    "    'CNEL_Average': 'Value',\n",
    "    \n",
    "}\n",
    "\n",
    "# Create a new DataFrame Final_df with columns based on the field mapping\n",
    "df = df.rename(columns=field_mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update SDE.Monitoring.sde.Plan_Area_Noise for Monitoring Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updated to make a feature class instead of a csv\n",
    "# Setup\n",
    "import pandas as pd\n",
    "from arcgis import GIS\n",
    "from arcgis.features import FeatureLayer\n",
    "from arcgis.features import GeoAccessor\n",
    "import os\n",
    "\n",
    "\n",
    "workspace =r'F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\n",
    "\n",
    "# Update SDE.Monitoring.SDE.Plan_Area_Noise with new data 2020-2023\n",
    "excel_file_path = r\"F:\\Research and Analysis\\Noise\\Monitoring\\CNEL - Plan Areas\\ALL_CNEL_PLANAREAS_RESULTS_(1991-2023).xlsx\"\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "Noise23_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Keep only columns that you want\n",
    "columns_to_keep = ['CNEL LAND USE', 'MONITORING SITE LATITUDE', 'MONITORING SITE LONGITUDE', 'PAS_NAME', 'LAND_USE', '2023 CNEL Maximum Day', '2022 CNEL Maximum Day', '2021 CNEL Maximum Day', '2020 CNEL Maximum Day']\n",
    "Noise23_df = Noise23_df[columns_to_keep]\n",
    "\n",
    "Noise23_df.columns = Noise23_df.columns.str.strip()\n",
    "\n",
    "# PASNAME\n",
    "Noise23_df['PAS_NAME'] = Noise23_df['PAS_NAME'].astype(str)\n",
    "\n",
    "# melted columns to keep\n",
    "meltcolumns_to_keep = ['CNEL LAND USE', 'MONITORING SITE LATITUDE', 'MONITORING SITE LONGITUDE', 'LAND_USE']\n",
    "id_columns = ['PAS_NAME']\n",
    "\n",
    "# Specify the columns to melt (exclude the identifier column)\n",
    "columns_to_melt = ['2023 CNEL Maximum Day', '2022 CNEL Maximum Day', '2021 CNEL Maximum Day', '2020 CNEL Maximum Day']\n",
    "\n",
    "id_vars = id_columns + meltcolumns_to_keep\n",
    "\n",
    "# Use pd.melt to reshape the DataFrame\n",
    "Noise23_df = pd.melt(Noise23_df, id_vars=id_vars, value_vars=columns_to_melt, var_name='Year', value_name='CNEL_Maximum_Day_Value')\n",
    "\n",
    "print(Noise23_df.head())\n",
    "print(Noise23_df.columns)\n",
    "\n",
    "# Extract only the year from the 'YEAR_OF_COUNT' column and drop \"CNEL Maximum Day\" from each value\n",
    "Noise23_df['Year'] = Noise23_df['Year'].str.replace(' CNEL Maximum Day', '')\n",
    "\n",
    "\n",
    "\n",
    "# Add 'Category' column\n",
    "Noise23_df['Category'] = 'CNEL Maximum Day'\n",
    "\n",
    "# Drop rows where 'CNEL_Maximum_Day_Value' is NaN\n",
    "Noise23_df = Noise23_df.dropna(subset=['CNEL_Maximum_Day_Value'])\n",
    "\n",
    "# Use lookup_dict to fill in LTINFO# Create a lookup dictionary to fill in LTINFO and ID\n",
    "gis = GIS()\n",
    "\n",
    "\n",
    "def get_fs_data(service_url):\n",
    "    feature_layer = FeatureLayer(service_url)\n",
    "    query_result = feature_layer.query()\n",
    "    feature_list = query_result.features\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    return all_data\n",
    "\n",
    "\n",
    "# REST SERVICE data for lookup dictionary\n",
    "service_url = 'https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/6'\n",
    "\n",
    "dfplannoisesde = get_fs_data(service_url)\n",
    "\n",
    "dfplannoisesde.info()\n",
    "columnstokeep = ['SITE_NAME', 'ID', 'LTINFO', 'PAS_1987', 'LATITUDE', 'LONGITUDE']\n",
    "dfplannoisesde = dfplannoisesde.loc[:, columnstokeep]\n",
    "\n",
    "# Drop duplicates based on 'SITE_NAME' and keep the first occurrence\n",
    "unique_values = dfplannoisesde.drop_duplicates(subset='SITE_NAME', keep='first')\n",
    "\n",
    "# Select specific columns for lookup\n",
    "selected_columns = ['SITE_NAME', 'ID', 'LTINFO', 'PAS_1987', 'LATITUDE', 'LONGITUDE']\n",
    "\n",
    "def update_lat_lon_from_lookup_dict(noise_data, lookup_dict):\n",
    "    # Assuming 'SITE_NAME' is the common identifier between DataFrame and lookup dictionary\n",
    "    for index, row in noise_data.iterrows():\n",
    "        # Get the 'LATITUDE' and 'LONGITUDE' from the lookup dictionary using the 'SITE_NAME' from the DataFrame\n",
    "        lat_lon = lookup_dict.get(row['PAS_NAME'])\n",
    "        # Update the 'MONITORING SITE LATITUDE' and 'MONITORING SITE LONGITUDE' in the DataFrame with the 'LATITUDE' and 'LONGITUDE' from the lookup dictionary\n",
    "        if lat_lon is not None:\n",
    "            noise_data.at[index, 'MONITORING SITE LATITUDE'] = lat_lon['LATITUDE']\n",
    "            noise_data.at[index, 'MONITORING SITE LONGITUDE'] = lat_lon['LONGITUDE']\n",
    "\n",
    "    \n",
    "    return noise_data\n",
    "\n",
    "# Convert selected columns to dictionary\n",
    "lookup_dict = unique_values[selected_columns].set_index('SITE_NAME').to_dict(orient='index')\n",
    "\n",
    "# Display the dictionary\n",
    "print(lookup_dict)\n",
    "\n",
    "# Update 'MONITORING SITE LATITUDE' and 'MONITORING SITE LONGITUDE' in Noise23_df with data from the lookup dictionary\n",
    "Noise23_df = update_lat_lon_from_lookup_dict(Noise23_df, lookup_dict)\n",
    "\n",
    "\n",
    "\n",
    "# Create new columns 'ID' and 'LTINFO' based on 'PAS_NAME' and look up dictionary\n",
    "Noise23_df['ID'] = Noise23_df['PAS_NAME'].apply(lambda x: lookup_dict.get(x, {}).get('ID'))\n",
    "Noise23_df['LTINFO'] = Noise23_df['PAS_NAME'].apply(lambda x: lookup_dict.get(x, {}).get('LTINFO'))\n",
    "Noise23_df['PAS_1987'] = Noise23_df['PAS_NAME'].apply(lambda x: lookup_dict.get(x, {}).get('PAS_1987'))\n",
    "\n",
    "# Specify the columns to keep in Final_sdf\n",
    "finalcolumns_to_keep = ['PAS_NAME', 'Year', 'CNEL_Maximum_Day_Value', 'Category','ID', 'MONITORING SITE LATITUDE', 'MONITORING SITE LONGITUDE', 'LAND_USE', 'CNEL LAND USE', 'LTINFO', 'PAS_1987']\n",
    "\n",
    "# Create a new DataFrame Final_df with the selected columns\n",
    "df = Noise23_df[finalcolumns_to_keep]\n",
    "\n",
    "# Field mapping so that columns match\n",
    "field_mapping = {\n",
    "    'PAS_1987': 'PAS_1987',\n",
    "    'ID': 'ID',\n",
    "    'MONITORING SITE LATITUDE': 'LATITUDE',\n",
    "    'MONITORING SITE LONGITUDE': 'LONGITUDE',\n",
    "    'PAS_NAME': 'SITE_NAME',\n",
    "    'LAND_USE': 'LAND_USE',\n",
    "    'CNEL LAND USE': 'CNEL_LAND_USE',\n",
    "    'Category': 'CATEGORY',\n",
    "    'Year': 'YEAR_OF_COUNT',\n",
    "    'CNEL_Maximum_Day_Value': 'COUNT_VALUE',\n",
    "    'LTINFO': 'LTINFO'\n",
    "}\n",
    "\n",
    "# Create a new DataFrame Final_df with columns based on the field mapping\n",
    "df = df.rename(columns=field_mapping)\n",
    "\n",
    "# Convert DataFrame to Spatially Enabled DataFrame\n",
    "sedf = GeoAccessor.from_xy(df, x_column='LONGITUDE', y_column='LATITUDE')\n",
    "\n",
    "# Convert the SEDF to a feature class without sanitizing columns\n",
    "sedf.spatial.to_featureclass(location=os.path.join(workspace, 'PlanAreaNoise_23_Staging'), sanitize_columns=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reproject feature class 4236 to 26910\n",
    "# Set the input and output paths\n",
    "arcpy.env.workspace=r'F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\n",
    "input_feature_class = 'PlanAreaNoise_23_Staging'\n",
    "output_feature_class = 'PlanAreaNoise_23_ready'\n",
    " \n",
    "# Specify the target coordinate system (EPSG code 26910 for UTM Zone 10N)\n",
    "target_coordinate_system = arcpy.SpatialReference(26910)\n",
    " \n",
    "# Use the Project tool to reproject the feature class\n",
    "arcpy.management.Project(\n",
    "    input_feature_class,\n",
    "    output_feature_class,\n",
    "    target_coordinate_system\n",
    ")\n",
    " \n",
    "print(f\"Reprojection completed. Output feature class: {output_feature_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.Delete_management(r'c:\\Users\\snewsome\\Documents\\GitHub\\ThresholdEvaluation\\Scripts\\2023\\db_connect\\ConnectionFile.sde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "from time import strftime\n",
    "\n",
    "\n",
    "#push data to sde\n",
    "#Create database connection\n",
    "inWorkspace = r\"F:\\Research and Analysis\\Workspace\\Sarah\\SarahVector.sde\"\n",
    "arcpy.env.workspace = inWorkspace\n",
    "\n",
    "# Specify the name of the new version and the parent version\n",
    "new_version_name = \"PlanAreaNoise_\" + strftime(\"%Y-%m-%d\")\n",
    "parent_version = \"sde.DEFAULT\"\n",
    "version_name_full = '\"TAHOE\\SNEWSOME\".'+ new_version_name\n",
    "\n",
    "# List all versions in the geodatabase\n",
    "existing_versions = [version.name for version in arcpy.da.ListVersions(inWorkspace)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "from time import strftime\n",
    "\n",
    "\n",
    "#push data to sde\n",
    "#Create database connection\n",
    "inWorkspace=r\"F:\\Research and Analysis\\Workspace\\Sarah\\SarahVector.sde\"\n",
    "arcpy.env.workspace=inWorkspace\n",
    "\n",
    "# Specify the name of the new version and the parent version\n",
    "new_version_name = \"PlanAreaNoise_\" + strftime(\"%Y-%m-%d\")\n",
    "parent_version = \"sde.DEFAULT\"\n",
    "version_name_full = '\"TAHOE\\SNEWSOME\".'+ new_version_name\n",
    "\n",
    "# List all versions in the geodatabase\n",
    "existing_versions = [version.name for version in arcpy.da.ListVersions(inWorkspace)]\n",
    "\n",
    "# Check if the specified version exists\n",
    "if version_name_full in existing_versions:\n",
    "    # Delete the version\n",
    "    arcpy.management.DeleteVersion(inWorkspace, version_name_full)\n",
    "    print(f\"Version '{version_name_full}' deleted successfully.\")\n",
    "else:\n",
    "    print(f\"Version '{version_name_full}' does not exist.\")\n",
    "\n",
    "# Create a new version\n",
    "arcpy.CreateVersion_management(inWorkspace, parent_version, new_version_name, \"PUBLIC\")\n",
    "\n",
    "# If you want to create a connection file, you can use the following code:\n",
    "arcpy.CreateDatabaseConnection_management(\n",
    "    out_folder_path='db_connect/',\n",
    "    out_name=\"ConnectionFile.sde\",\n",
    "    database_platform=\"SQL_SERVER\",  \n",
    "    instance='sql12',\n",
    "    database='sde',\n",
    "    account_authentication=\"OPERATING_SYSTEM_AUTH\",  \n",
    "    version_type='TRANSACTIONAL',\n",
    "    version=version_name_full\n",
    ")\n",
    "\n",
    "PlanNoisesde=os.path.join(version_name_full, \"SDE.Monitoring\\SDE.Plan_Area_Noise\")\n",
    "PlanAreaNoise = 'planareanoise'\n",
    "arcpy.MakeFeatureLayer_management(PlanNoisesde, PlanAreaNoise)\n",
    "\n",
    "\n",
    "# Create a new version\n",
    "#arcpy.CreateVersion_management(inWorkspace, parent_version, new_version_name, \"PUBLIC\")\n",
    "arcpy.ChangeVersion_management(PlanAreaNoise, 'TRANSACTIONAL', version_name_full, '')\n",
    "\n",
    "#Start\n",
    "edit = arcpy.da.Editor('db_connections/ConnectionFile.sde')\n",
    "edit.startEditing(False, True)\n",
    "\n",
    "# Append the records from the temporary feature class to the target feature class\n",
    "arcpy.management.Append(\n",
    "    inputs='PlanAreaNoise_23_ready',\n",
    "    target=PlanNoisesde, \n",
    "    schema_type='NO_TEST'\n",
    ")\n",
    "\n",
    "#Stop\n",
    "edit.stopEditing(True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    arcpy.CreateVersion_management(inWorkspace, parent_version, new_version_name, \"PUBLIC\")\n",
    "    print(\"Version created successfully.\")\n",
    "except arcpy.ExecuteError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start\n",
    "edit = arcpy.da.Editor('db_connections/ConnectionFile.sde')\n",
    "edit.startEditing(False, True)\n",
    "\n",
    "# Append the records from the temporary feature class to the target feature class\n",
    "arcpy.management.Append(\n",
    "    inputs='PlanAreaNoise_23_ready',\n",
    "    target=os.path.join(version_full_name, \"SDE.Monitoring\\SDE.Plan_Area_Noise\"), \n",
    "    schema_type='NO_TEST'\n",
    ")\n",
    "\n",
    "#Stop\n",
    "edit.stopEditing(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.Delete_management(\"planareanoise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start an edit session\n",
    "edit = arcpy.da.Editor('db_connections/ConnectionFile.sde')\n",
    "edit.startEditing(False, True)\n",
    "\n",
    "    # Create a new version\n",
    "arcpy.CreateVersion_management(inWorkspace, parent_version, new_version_name, \"PUBLIC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shorezone Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream/CSCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the Average CSCI score for TPAandTPBsites for 2020-2022 and fill in csv, Next code block will be splitting TPA and TPB into two different avg scores\n",
    "#Setup\n",
    "#Create Dictionary Usring Rest Service data\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from arcgis import GIS\n",
    "from arcgis.features import FeatureLayer\n",
    "import numpy as np\n",
    "import requests\n",
    "import arcpy\n",
    "\n",
    "gis = GIS()\n",
    "# Connect to TRPA Enterprise GIS Portal *if it's a service only shared with org\n",
    "# portal_user = \"TRPA_PORTAL_ADMIN\"\n",
    "# portal_pwd = str(os.environ.get('Password'))\n",
    "# portal_url = \"https://maps.trpa.org/portal/\"\n",
    "wk_memory = \"memory\" + \"\\\\\"\n",
    "\n",
    "#Set up Workspace\n",
    "# set workspace and sde connections \n",
    "working_folder = r\"F:\\Research and Analysis\\Threshold reporting\\ThresholdData\\Fisheries\"\n",
    "workspace      = \"C:\\GIS\\Scratch.gdb\"\n",
    "arcpy.env.workspace = \"C:\\GIS\\Scratch.gdb\"\n",
    "\n",
    "# network path to connection files\n",
    "filePath = r\"C:\\\\GIS\\\\DB_CONNECT\"\n",
    "\n",
    "\n",
    "## CSV to be updated with new average\n",
    "StreamAvg = os.path.join(working_folder,\"ThresholdData_Stream.csv\")\n",
    "\n",
    "\n",
    "def get_fs_data(service_url):\n",
    "    feature_layer = FeatureLayer(service_url)\n",
    "    query_result = feature_layer.query()\n",
    "    # Convert the query result to a list of dictionaries\n",
    "    feature_list = query_result.features\n",
    "    # Create a pandas DataFrame from the list of dictionaries\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    return all_data\n",
    "\n",
    "# Service URL\n",
    "service_url = 'https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/8'\n",
    "\n",
    "# Get Stream data as a Spatially Enabled DataFrame\n",
    "sdfStreamHab = get_fs_data(service_url)\n",
    "\n",
    "# Keep only necessary columns\n",
    "columnstokeep = ['SITE_NAME', 'STATION_TYPE', 'DURATION', 'LATITUDE', 'LONGITUDE', 'LTINFO', 'COUNT_VALUE', 'YEAR_OF_COUNT']\n",
    "sdfStreamHab = sdfStreamHab.loc[:, columnstokeep]\n",
    "\n",
    "# Query only 2020-2022 data\n",
    "sdfStreamHab['Date'] = pd.to_datetime(sdfStreamHab['YEAR_OF_COUNT'], format='%Y')\n",
    "filtered_df = sdfStreamHab[\n",
    "    (sdfStreamHab['Date'].dt.year.between(2020, 2022)) & \n",
    "    (sdfStreamHab['SITE_NAME'].str.contains('TPB|TPA'))\n",
    "].copy()\n",
    "\n",
    "# Calculate the average of the 'COUNT_VALUE' column\n",
    "AvgCSCI23 = filtered_df['COUNT_VALUE'].mean()\n",
    "\n",
    "# Update CSV ThresholdData_Stream.csv\n",
    "csv_path = r\"F:\\Research and Analysis\\Threshold reporting\\ThresholdData\\Fisheries\\ThresholdData_Streams.csv\"\n",
    "existing_df = pd.read_csv(csv_path)\n",
    "\n",
    "Description = \"Average CSCI score of all trend sites (48 sites)\"\n",
    "Year = '2020-2022'\n",
    "Value = AvgCSCI\n",
    "\n",
    "existing_entry = existing_df[\n",
    "    (existing_df['Description'] == Description) &\n",
    "    (existing_df['Year'] == Year)\n",
    "]\n",
    "\n",
    "if existing_entry.empty:\n",
    "    new_entry = {'Description': Description, 'Year': Year, 'Value': Value}\n",
    "    existing_df = pd.concat([existing_df, pd.DataFrame([new_entry])], ignore_index=True)\n",
    "\n",
    "\n",
    "existing_df.to_csv(csv_path, index=False)\n",
    "print(f\"CSV file '{csv_path}' has been updated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avg Stream CSCI split into TPB and TPA for all years. in a new excel sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from arcgis import GIS\n",
    "from arcgis.features import FeatureLayer\n",
    "import numpy as np\n",
    "\n",
    "import arcpy\n",
    "\n",
    "gis = GIS()\n",
    "\n",
    "wk_memory = \"memory\" + \"\\\\\"\n",
    "\n",
    "#Set up Workspace\n",
    "# set workspace and sde connections \n",
    "working_folder = r\"F:\\Research and Analysis\\Threshold reporting\\ThresholdData\\Fisheries\"\n",
    "workspace      = \"F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb\"\n",
    "arcpy.env.workspace = \"F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb\"\n",
    "\n",
    "\n",
    "\n",
    "## excel to be updated with new averages split into TPA and TPB\n",
    "StreamAvg = os.path.join(working_folder,\"ThresholdData_StreamsTavg.xlsx\")\n",
    "\n",
    "\n",
    "def get_fs_data(service_url):\n",
    "    feature_layer = FeatureLayer(service_url)\n",
    "    query_result = feature_layer.query()\n",
    "    # Convert the query result to a list of dictionaries\n",
    "    feature_list = query_result.features\n",
    "    # Create a pandas DataFrame from the list of dictionaries\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    return all_data\n",
    "\n",
    "# Service URL\n",
    "service_url = 'https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/8'\n",
    "\n",
    "# Get Stream data as REST SERVICE to a Spatially Enabled DataFrame\n",
    "sdfStreamHab = get_fs_data(service_url)\n",
    "\n",
    "# Keep only necessary columns\n",
    "columnstokeep = ['SITE_NAME', 'STATION_TYPE', 'DURATION', 'LATITUDE', 'LONGITUDE', 'LTINFO', 'COUNT_VALUE', 'YEAR_OF_COUNT']\n",
    "sdfStreamHab = sdfStreamHab.loc[:, columnstokeep]\n",
    "\n",
    "#This is where the magic calculations happen for each threshold\n",
    "# Mke year of count into date format\n",
    "sdfStreamHab['Date'] = pd.to_datetime(sdfStreamHab['YEAR_OF_COUNT'], format='%Y')\n",
    "\n",
    "# Define bins for grouping by year ranges\n",
    "bins = [2010, 2012, 2014, 2016, 2020, 2023]\n",
    "\n",
    "# Define labels for the bins\n",
    "labels = ['2010-2011', '2012-2013', '2014-2015', '2016-2019', '2020-2022']\n",
    "\n",
    "# Create a new column 'YearRange' to group the data by year ranges\n",
    "sdfStreamHab['YearRange'] = pd.cut(sdfStreamHab['Date'].dt.year, bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Filter the DataFrame for TPB sites and TPA sites\n",
    "filtered_df_tpb = sdfStreamHab[sdfStreamHab['SITE_NAME'].str.contains('TPB')]\n",
    "filtered_df_tpa = sdfStreamHab[sdfStreamHab['SITE_NAME'].str.contains('TPA')]\n",
    "\n",
    "# Group by 'YearRange', then calculate the average 'COUNT_VALUE' TPB sites\n",
    "averages_tpb = filtered_df_tpb.groupby('YearRange')['COUNT_VALUE'].mean().reset_index()\n",
    "averages_tpb.rename(columns={'COUNT_VALUE': 'Average_COUNT_VALUE_TPB'}, inplace=True)\n",
    "\n",
    "averages_tpa = filtered_df_tpa.groupby('YearRange')['COUNT_VALUE'].mean().reset_index()\n",
    "averages_tpa.rename(columns={'COUNT_VALUE': 'Average_COUNT_VALUE_TPA'}, inplace=True)\n",
    "\n",
    "# Create Description and Year columns in both DataFrames\n",
    "averages_tpb['Description'] = 'Average CSCI Score per Trend Panel(24 Sites)'\n",
    "averages_tpa['Description'] = 'Average CSCI Score per Trend Panel(24 Sites)'\n",
    "averages_tpb['Year'] = averages_tpb['YearRange']\n",
    "averages_tpa['Year'] = averages_tpa['YearRange']\n",
    "\n",
    "# Merge the two DataFrames on 'Year' and 'Description' to create the final merged DataFrame\n",
    "FAdf = pd.merge(averages_tpb, averages_tpa, on=['Year', 'Description'], how='outer')\n",
    "# Sort by 'Year' for clarity\n",
    "FAdf.sort_values('Year', inplace=True)\n",
    "\n",
    "# Reset index for clarity\n",
    "FAdf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Define columns to keep\n",
    "columns_to_keep = ['Year', 'Description', 'Average_COUNT_VALUE_TPB', 'Average_COUNT_VALUE_TPA']\n",
    "\n",
    "# Select only the desired columns\n",
    "FAdf = FAdf[columns_to_keep]\n",
    "# Print the combined averages\n",
    "print(FAdf)\n",
    "print(\"FAdf:\")\n",
    "\n",
    "# Write the updated DataFrame back to the CSV file\n",
    "FAdf.to_excel(StreamAvg, index=False)\n",
    "\n",
    "print(f\"The excel file '{StreamAvg}' has been updated with new averages successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use F:Research and Analysis/Threshold reporting\\ThresholdData\\Fisheries\\ThresholdData_Streams.csv' to update SDE.tabular.....table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Rating or CSCI values and then find the percent of total streams in each rating\n",
    "excel_file_path=\"F:\\Research and Analysis\\Fisheries\\Streams\\Bioassessment\\California Stream Condition Index\\California Stream Condition Index\\CSCI_Scores_AllSites_AllYears.xlsx\"\n",
    "\n",
    "Alldf = pd.read_excel(excel_file_path, sheet_name='CSCI_Scores_AllSites_AllYears')\n",
    "\n",
    "#Calculate Rating for CSCI value\n",
    "#Define a function to categorize CSCI values based on ranges\n",
    "def categorize_value(value):\n",
    "    if 0 <= value < 0.8:\n",
    "        return 'marginal'\n",
    "    elif 0.8 <= value < 1.0:\n",
    "        return 'good'\n",
    "    else:\n",
    "        return 'excellent'\n",
    "\n",
    "Alldf['Rating']=Alldf['CSCI_Score'].apply(categorize_value)\n",
    "\n",
    "print(Alldf)\n",
    "\n",
    "#Find average for each rating- Poor, Marginal, Good, Excellent\n",
    "# Group by 'Category' and calculate the mean of 'Count'\n",
    "average_count_by_category = df.groupby('Rating')['CSCI_Score'].mean()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wild Life"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Wildlifedf['Wildlife_Species'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wildlife Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import pandas as pd\n",
    "import os\n",
    "# Delete the table\n",
    "\n",
    "workspace = r'F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\n",
    "#arcpy.Delete_management(os.path.join(workspace, 'Wildlife_Staging'))\n",
    "# Update SDE.tabular.SDE.threshhold_PlanAreaNoise with new data 2020-2023\n",
    "excel_file_path = r\"F:\\Research and Analysis\\Threshold reporting\\ThresholdData\\Wildlife\\Wildlife_ThresholdData.xlsx\"\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "Rawdf = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Grab needed data by year\n",
    "Wildlifedf = Rawdf.loc[(Rawdf['Year'] >= 2020) & (Rawdf['Year'] <= 2023)].copy()\n",
    "\n",
    "#Rename columns directly\n",
    "Wildlifedf.rename(columns={'Wildlife Species': 'Wildlife_Species', 'Threshold Value': 'Threshold_Value', 'Count': 'Total'}, inplace=True)\n",
    "# Convert 'Wildlife_Species' column to string type\n",
    "Wildlifedf['Wildlife_Species'] = Wildlifedf['Wildlife_Species'].astype(str)\n",
    "# Define the fields to include in the table\n",
    "field_names = ['Wildlife_Species', 'Threshold_Value', 'Category', 'Year', 'Total']  # Add more fields if necessary\n",
    "\n",
    "# Convert 'Wildlife Species' column to string type to ensure compatibility\n",
    "Wildlifedf['Wildlife_Species'] = Wildlifedf['Wildlife_Species'].astype(str)\n",
    "\n",
    "# Define the fields to include in the table with specified field types\n",
    "field_types = {'Wildlife_Species': 'Text', 'Threshold_Value': 'Text', 'Category': 'TEXT', 'Year': 'Double', 'Total': 'Double'}\n",
    "\n",
    "# Create an empty table in the workspace\n",
    "table_path = os.path.join(workspace, 'Wildlife_staging')\n",
    "arcpy.CreateTable_management(workspace, 'Wildlife_staging')\n",
    "\n",
    "# Add fields to the table with specified field types\n",
    "for field_name, field_type in field_types.items():\n",
    "    arcpy.AddField_management(table_path, field_name, field_type)\n",
    "\n",
    "# Convert the DataFrame to a structured numpy array\n",
    "array = Wildlifedf[list(field_types.keys())].to_records(index=False)\n",
    "\n",
    "# Insert data into the table\n",
    "with arcpy.da.InsertCursor(table_path, list(field_types.keys())) as cursor:\n",
    "    for row in array:\n",
    "        cursor.insertRow(row)\n",
    "\n",
    "\n",
    "        #GO TO WORKSPACE FOLDER AND APPEND TO F:\\GIS\\DB_CONNECT\\Tabular.sde\\SDE.ThresholdEvaluation_Wildlife in PRO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waterfowl Human Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not done\n",
    "import arcpy\n",
    "import pandas as pd\n",
    "import os\n",
    "# Delete the table\n",
    "\n",
    "workspace = r'F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\n",
    "#arcpy.Delete_management(os.path.join(workspace, 'Wildlife_Staging'))\n",
    "# Update SDE.tabular.SDE.threshhold_PlanAreaNoise with new data 2020-2023\n",
    "excel_file_path = r\"F:\\Research and Analysis\\Threshold reporting\\ThresholdData\\Wildlife\\Waterfowl_HumanActivityRating_2019.xlsx\"\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "Rawdf = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Grab needed data by year\n",
    "Wildlifedf = Rawdf.loc[(Rawdf['Year'] >= 2020) & (Rawdf['Year'] <= 2023)].copy()\n",
    "\n",
    "#Rename columns directly\n",
    "fowldf.rename(columns={'Wildlife Species': 'Wildlife_Species', 'Threshold Value': 'Threshold_Value', 'Count': 'Total'}, inplace=True)\n",
    "# Convert 'Wildlife_Species' column to string type\n",
    "fowldf['Wildlife_Species'] = fowldf['Wildlife_Species'].astype(str)\n",
    "# Define the fields to include in the table\n",
    "field_names = ['Wildlife_Species', 'Threshold_Value', 'Category', 'Year', 'Total']  # Add more fields if necessary\n",
    "\n",
    "# Convert 'Wildlife Species' column to string type to ensure compatibility\n",
    "Wildlifedf['Wildlife_Species'] = Wildlifedf['Wildlife_Species'].astype(str)\n",
    "\n",
    "# Define the fields to include in the table with specified field types\n",
    "field_types = {'Wildlife_Species': 'Text', 'Threshold_Value': 'Text', 'Category': 'TEXT', 'Year': 'Double', 'Total': 'Double'}\n",
    "\n",
    "# Create an empty table in the workspace\n",
    "table_path = os.path.join(workspace, 'Wildlife_staging')\n",
    "arcpy.CreateTable_management(workspace, 'Wildlife_staging')\n",
    "\n",
    "# Add fields to the table with specified field types\n",
    "for field_name, field_type in field_types.items():\n",
    "    arcpy.AddField_management(table_path, field_name, field_type)\n",
    "\n",
    "# Convert the DataFrame to a structured numpy array\n",
    "array = Wildlifedf[list(field_types.keys())].to_records(index=False)\n",
    "\n",
    "# Insert data into the table\n",
    "with arcpy.da.InsertCursor(table_path, list(field_types.keys())) as cursor:\n",
    "    for row in array:\n",
    "        cursor.insertRow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wildlife Data confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REST Service that refers to ['sde_tabular.SDE.ThresholdEvaluation_Waterfowl']\n",
    "\n",
    "#['sde_tabular.SDE.ThresholdEvaluation_Wildlife']\n",
    "def get_fs_data(wildlife_url):\n",
    "    feature_layer = FeatureLayer(wildlife_url)\n",
    "    query_result = feature_layer.query()\n",
    "    feature_list = query_result.features\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    return all_data\n",
    "\n",
    "waterfowl_url = \"https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/94\"\n",
    "wildlife_url = \"https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/96\"\n",
    "draftworkspace = r\"C:\\Users\\snewsome\\Documents\\GitHub\\ThresholdEvaluation\\2023\\Wildlife\\Chart\\Draft\"\n",
    "dfWildlife = get_fs_data(wildlife_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wintering Bald Eagle linear regression confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress, t\n",
    "\n",
    "# Setup dataframe\n",
    "df = dfWildlife.loc[dfWildlife['Wildlife_Species'] == 'Bald Eagle - winter']\n",
    "\n",
    "# Calculate linear regression using scipy's linregress\n",
    "slope, intercept, r_value, p_value, std_err = linregress(df['Year'], df['Total'])\n",
    "r_squared = r_value ** 2\n",
    "\n",
    "# Calculate the degrees of freedom\n",
    "n = len(df)\n",
    "dof = n - 2\n",
    "\n",
    "# Calculate two-tailed t-value for 95% confidence interval\n",
    "t_value = t.ppf(0.975, dof)  # 0.975 for a two-tailed 95% confidence level\n",
    "\n",
    "# Calculate confidence interval for the slope\n",
    "slope_ci_lower = slope - t_value * std_err\n",
    "slope_ci_upper = slope + t_value * std_err\n",
    "\n",
    "\n",
    "\n",
    "# Display results\n",
    "print(f\"Slope: {slope}\")\n",
    "print(f\"95% Confidence Interval for Slope: [{slope_ci_lower}, {slope_ci_upper}]\")\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "print(f\"dof: {dof}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Osprey 95 confidence interval of linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress, t\n",
    "\n",
    "# setup dataframe\n",
    "df = dfWildlife.loc[dfWildlife['Wildlife_Species'] == 'Osprey']\n",
    "\n",
    "# Calculate linear regression using scipy's linregress\n",
    "slope, intercept, r_value, p_value, std_err = linregress(df['Year'], df['Total'])\n",
    "r_squared = r_value ** 2\n",
    "\n",
    "# Calculate the degrees of freedom\n",
    "n = len(df)\n",
    "dof = n - 2\n",
    "\n",
    "# Calculate two-tailed t-value for 95% confidence interval\n",
    "t_value = t.ppf(0.975, dof)  # 0.975 for a two-tailed 95% confidence level\n",
    "\n",
    "# Calculate confidence interval for the slope\n",
    "slope_ci_lower = slope - t_value * std_err\n",
    "slope_ci_upper = slope + t_value * std_err\n",
    "\n",
    "# Display results\n",
    "print(f\"Slope: {slope}\")\n",
    "print(f\"95% Confidence Interval for Slope: [{slope_ci_lower}, {slope_ci_upper}]\")\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Falcon 95 confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress, t\n",
    "\n",
    "# setup dataframe\n",
    "df = dfWildlife.loc[dfWildlife['Wildlife_Species'] == 'Peregrine Falcon']\n",
    "\n",
    "# Calculate linear regression using scipy's linregress\n",
    "slope, intercept, r_value, p_value, std_err = linregress(df['Year'], df['Total'])\n",
    "r_squared = r_value ** 2\n",
    "\n",
    "# Calculate the degrees of freedom\n",
    "n = len(df)\n",
    "dof = n - 2\n",
    "\n",
    "# Calculate two-tailed t-value for 95% confidence interval\n",
    "t_value = t.ppf(0.975, dof)  # 0.975 for a two-tailed 95% confidence level\n",
    "\n",
    "# Calculate confidence interval for the slope\n",
    "slope_ci_lower = slope - t_value * std_err\n",
    "slope_ci_upper = slope + t_value * std_err\n",
    "\n",
    "# Display results\n",
    "print(f\"Slope: {slope}\")\n",
    "print(f\"95% Confidence Interval for Slope: [{slope_ci_lower}, {slope_ci_upper}]\")\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watercraft Inspections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "url = 'https://www.laketahoeinfo.org/WebServices/GetReportedEIPIndicatorProjectAccomplishments/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476/16'\n",
    "\n",
    "df = pd.read_json(url)\n",
    "df\n",
    "\n",
    "# stacked bar chart with plotly\n",
    "fig = px.bar(df, x='IndicatorProjectYear', y='IndicatorProjectValue', color='PMSubcategoryOption1', barmode='stack')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate 3 year mean for each year for PM 10 Annual and PM 2.5 Annual\n",
    "\n",
    "# Use rest service to get data for Air Quality\n",
    "#Setup\n",
    "import pandas as pd\n",
    "import os\n",
    "from arcgis.features import FeatureLayer\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def get_fs_data(fs_url):\n",
    "    feature_layer = FeatureLayer(fs_url)\n",
    "    query_result = feature_layer.query()\n",
    "    feature_list = query_result.features\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    return all_data\n",
    "\n",
    "\n",
    "\n",
    "Airservice_url = \"https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/46\"\n",
    "dfAir = get_fs_data(Airservice_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annual PM 2.5 3 year avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set indicator\n",
    "indicator = 'PM2.5 - ANNUAL AVG.'\n",
    "# limit rows to indicator\n",
    "dfPM25 = dfAir.loc[dfAir['Indicator'] == indicator]\n",
    "# correct threshold value errors\n",
    "dfPM25['Threshold Value'] = 9\n",
    "\n",
    "# setup dataframe\n",
    "df = dfPM25\n",
    "# Ensure data is sorted by year if needed\n",
    "df = df.sort_values(by='Year')\n",
    "#Rolling Average per station per year\n",
    "dfPM25['Site_3_Year_Average'] = dfPM25.groupby('Site')['Value'].rolling(window=3).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "dfPM25.to_csv('station_3_year_average.csv', index=False)\n",
    "\n",
    "# Calculate the yearly average across stations\n",
    "yearly_avg = dfPM25.groupby('Year')['Value'].mean().reset_index()\n",
    "\n",
    "# Calculate the 3-year rolling average on the yearly data\n",
    "yearly_avg['3_Year_Average'] = yearly_avg['Value'].rolling(window=3).mean()\n",
    "\n",
    "# Creating DataFrame\n",
    "3 year average_df = pd.DataFrame({\n",
    "    'Category' : ' High 24hr PM2.5',\n",
    "    'Year': '2023 Threshold Evaluation',\n",
    "    'R_Squared': r_squared,\n",
    "    'P_Value': p_value,\n",
    "    'Trendline Slope': slope,\n",
    "    'HTML URL': 'https://trpa-agency.github.io/ThresholdEvaluation/2023/AirQuality/Chart/Draft/AnalysisAirQuality_24hrPM25.html'\n",
    "}, index=[0])\n",
    "\n",
    "# Define the path to save the CSV file\n",
    "#csv_file_path = os.path.join(analysis_final, 'AirQuality_Trendlines.csv')\n",
    "# Write DataFrame to CSV file\n",
    "print(\"DataFrame to be written to CSV:\")\n",
    "print(slope_df)\n",
    "# Write DataFrame to CSV file\n",
    "#slope_df.to_csv(csv_file_path, mode='a', header=False, index=False)\n",
    "\n",
    "# Calculate the 3-year rolling average for the 'Value' column\n",
    "#df['3_Year_Average'] = df['Value'].rolling(window=3).mean()\n",
    "\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# set indicator\n",
    "indicator = 'PM2.5 - ANNUAL AVG.'\n",
    "# limit rows to indicator\n",
    "dfPM25 = dfAir.loc[dfAir['Indicator'] == indicator]\n",
    "# correct threshold value errors\n",
    "dfPM25['Threshold Value'] = 9\n",
    "\n",
    "# setup dataframe\n",
    "df = dfPM25\n",
    "\n",
    "\n",
    "# Step 1: Calculate the 3-year rolling average per station\n",
    "dfPM25['3_Year_Average'] = dfPM25.groupby('Site')['Value'].rolling(window=3).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# Step 2: Calculate the yearly average across all stations\n",
    "yearly_avg = dfPM25.groupby('Year')['Value'].mean().reset_index()\n",
    "yearly_avg['Site'] = 'All Sites'  # Create an 'All Sites' label\n",
    "yearly_avg['3_Year_Average'] = yearly_avg['Value'].rolling(window=3).mean()  # Rolling average for all stations\n",
    "\n",
    "# Step 3: Combine the station data with the overall average\n",
    "combined_df = pd.concat([dfPM25[['Year', 'Site', '3_Year_Average']], yearly_avg[['Year', 'Site', '3_Year_Average']]])\n",
    "\n",
    "# Save to CSV\n",
    "combined_df.to_csv(r'C:\\Users\\snewsome\\Documents\\Threshold\\combined_3_year_average.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pm 10 annual 3 year rolling average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# set indicator\n",
    "indicator = 'PM10 - ANNUAL AVG.'\n",
    "# limit rows to indicator\n",
    "dfPM10 = dfAir.loc[dfAir['Indicator'] == indicator]\n",
    "# correct threshold value errors\n",
    "dfPM10['Threshold Value'] = 20\n",
    "\n",
    "# setup dataframe\n",
    "df = dfPM10\n",
    "\n",
    "\n",
    "# Step 1: Calculate the 3-year rolling average per station\n",
    "dfPM10['3_Year_Average'] = dfPM10.groupby('Site')['Value'].rolling(window=3).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Calculate the yearly average across all stations\n",
    "yearly_avg = dfPM10.groupby('Year')['Value'].mean().reset_index()\n",
    "yearly_avg['Site'] = 'All Sites'  # Create an 'All Sites' label\n",
    "yearly_avg['3_Year_Average'] = yearly_avg['Value'].rolling(window=3).mean()  # Rolling average for all stations\n",
    "\n",
    "# Step 3: Combine the station data with the overall average\n",
    "combined_df = pd.concat([dfPM10[['Year', 'Site', '3_Year_Average']], yearly_avg[['Year', 'Site', '3_Year_Average']]])\n",
    "\n",
    "# Save to CSV\n",
    "combined_df.to_csv(r'C:\\Users\\snewsome\\Documents\\Threshold\\pm10combined_3_year_average.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PM 10 Annual Chart 3 year rolling average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Load the combined data\n",
    "#combined_df = pd.read_csv(r'C:\\Users\\snewsome\\Documents\\Threshold\\pm10combined_3_year_average.csv')\n",
    "# set indicator\n",
    "indicator = 'PM10 - ANNUAL AVG.'\n",
    "# limit rows to indicator\n",
    "dfPM10 = dfAir.loc[dfAir['Indicator'] == indicator]\n",
    "# correct threshold value errors\n",
    "dfPM10['Threshold Value'] = 20\n",
    "\n",
    "# setup dataframe\n",
    "df = dfPM10\n",
    "\n",
    "\n",
    "# Step 1: Calculate the 3-year rolling average per station\n",
    "dfPM10['3_Year_Average'] = dfPM10.groupby('Site')['Value'].rolling(window=3).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Calculate the yearly average across all stations\n",
    "yearly_avg = dfPM10.groupby('Year')['Value'].mean().reset_index()\n",
    "yearly_avg['Site'] = 'All Sites'  # Create an 'All Sites' label\n",
    "yearly_avg['3_Year_Average'] = yearly_avg['Value'].rolling(window=3).mean()  # Rolling average for all stations\n",
    "\n",
    "# Step 3: Combine the station data with the overall average\n",
    "combined_df = pd.concat([dfPM10[['Year', 'Site', '3_Year_Average']], yearly_avg[['Year', 'Site', '3_Year_Average']]])\n",
    "# Chart 1: Rolling 3-year average for all sites\n",
    "all_sites_df = combined_df[combined_df['Site'] == 'All Sites']\n",
    "fig_all_sites = px.scatter(\n",
    "    all_sites_df,\n",
    "    x='Year',\n",
    "    y='3_Year_Average',\n",
    "    title='PM10 Annual - 3-Year Rolling Average for All Sites',\n",
    "    labels={'3_Year_Average': '3-Year Average (ppm)', 'Year': 'Year'}\n",
    ")\n",
    "fig_all_sites.update_traces(hovertemplate='<br>%{y:.2f} ppm')\n",
    "\n",
    "# Add the threshold line\n",
    "threshold_value = 20  # Replace with the appropriate threshold value if it's static, or use data if it varies.\n",
    "fig_all_sites.add_trace(go.Scatter(\n",
    "    x=all_sites_df['Year'],\n",
    "    y=[threshold_value] * len(all_sites_df),  # Replace with threshold column if values vary by year\n",
    "    name=\"Threshold\",\n",
    "    line=dict(color='#333333', width=3),\n",
    "    mode='lines',\n",
    "    hovertemplate='Threshold:<br>%{y:.2f} ppm<extra></extra>'\n",
    "))\n",
    "\n",
    "# Calculate and plot the trendline\n",
    "fig_trend = px.scatter(all_sites_df, x='Year', y='3_Year_Average', trendline='ols', trendline_color_override='#8a7121')\n",
    "trendline_trace = fig_trend.data[1]  # Get the trendline trace\n",
    "\n",
    "# Obtain trendline slope (beta) for display in hovertemplate\n",
    "fit_results = px.get_trendline_results(fig_trend).px_fit_results.iloc[0]\n",
    "beta = fit_results.params[1]\n",
    "slope = beta\n",
    "\n",
    "# Update trendline with additional details and add to main figure\n",
    "trendline_trace.update(\n",
    "    showlegend=True,\n",
    "    name=\"Trend\",\n",
    "    line=dict(width=3),\n",
    "    customdata=[slope] * len(trendline_trace['y']),\n",
    "    hovertemplate='Trend:<br>Slope %{customdata:.2f}<extra></extra>'\n",
    ")\n",
    "fig_all_sites.add_trace(trendline_trace)\n",
    "\n",
    "# Show the combined chart\n",
    "fig_all_sites.show()\n",
    "\n",
    "# save to HTML\n",
    "fig.write_html(os.path.join(draftworkspace, \"AirQuality_AnnualPM10_3YrAvg_allsites.html\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Load the combined data\n",
    "combined_df = pd.read_csv('C:\\\\Users\\\\snewsome\\\\Documents\\\\Threshold\\\\pm10combined_3_year_average.csv')\n",
    "\n",
    "# Filter out \"All Sites\" to get only individual site data\n",
    "per_site_df = combined_df[combined_df['Site'] != 'All Sites']\n",
    "\n",
    "# Create a scatter plot for each site's rolling 3-year average\n",
    "fig_per_site = px.scatter(\n",
    "    per_site_df,\n",
    "    x='Year',\n",
    "    y='3_Year_Average',\n",
    "    color='Site',\n",
    "    title='PM10 Annual - 3-Year Rolling Average Per Site',\n",
    "    labels={'3_Year_Average': '3-Year Average (ppm)', 'Year': 'Year'},\n",
    "    hover_data={'Site': True}\n",
    ")\n",
    "\n",
    "# Update the hover template for each trace\n",
    "fig_per_site.update_traces(hovertemplate='<br>%{y:.2f} ppm')\n",
    "\n",
    "# Add a threshold line across all sites\n",
    "threshold_value = 20  \n",
    "fig_per_site.add_trace(go.Scatter(\n",
    "    x=per_site_df['Year'].unique(),  # Unique years for the x-axis\n",
    "    y=[threshold_value] * len(per_site_df['Year'].unique()),  # Threshold line across all years\n",
    "    name=\"Threshold\",\n",
    "    line=dict(color='#333333', width=3),\n",
    "    mode='lines',\n",
    "    hovertemplate='Threshold:<br>%{y:.2f} ppm<extra></extra>'\n",
    "))\n",
    "\n",
    "# Calculate and add trendlines per site\n",
    "for site in per_site_df['Site'].unique():\n",
    "    site_data = per_site_df[per_site_df['Site'] == site]\n",
    "    \n",
    "    # Calculate the slope and intercept using linregress\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(site_data['Year'], site_data['3_Year_Average'])\n",
    "    \n",
    "    # Scatter plot with OLS trendline for each site\n",
    "    fig_site_trend = px.scatter(site_data, x='Year', y='3_Year_Average', trendline='ols', trendline_color_override='#8a7121')\n",
    "    \n",
    "    # Extract the trendline trace for the site\n",
    "    trendline_trace = fig_site_trend.data[1]\n",
    "    \n",
    "    # Update trendline trace with the calculated slope value in the hover template\n",
    "    trendline_trace.update(\n",
    "        showlegend=True,\n",
    "        name=f\"{site} Trend\",\n",
    "        line=dict(width=3),\n",
    "        hovertemplate=f'{site} Trend:<br>Slope {slope:.2f}<extra></extra>'  # Fixed slope value in hover template\n",
    "    )\n",
    "    \n",
    "    # Add the trendline trace to the main figure\n",
    "    fig_per_site.add_trace(trendline_trace)\n",
    "\n",
    "# Display the chart\n",
    "fig_per_site.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Load the combined data\n",
    "combined_df = pd.read_csv('C:\\\\Users\\\\snewsome\\\\Documents\\\\Threshold\\\\pm10combined_3_year_average.csv')\n",
    "\n",
    "# Create a scatter plot for all sites' rolling 3-year averages\n",
    "fig_per_site = px.scatter(\n",
    "    combined_df,\n",
    "    x='Year',\n",
    "    y='3_Year_Average',\n",
    "    color='Site',\n",
    "    title='PM10 Annual - 3-Year Rolling Average for All Sites',\n",
    "    labels={'3_Year_Average': '3-Year Average (ppm)', 'Year': 'Year'},\n",
    "    hover_data={'Site': True}\n",
    ")\n",
    "\n",
    "# Update the hover template for each trace\n",
    "fig_per_site.update_traces(hovertemplate='<br>%{y:.2f} ppm')\n",
    "\n",
    "# Add a threshold line across all sites\n",
    "threshold_value = 20  \n",
    "fig_per_site.add_trace(go.Scatter(\n",
    "    x=combined_df['Year'].unique(),  # Unique years for the x-axis\n",
    "    y=[threshold_value] * len(combined_df['Year'].unique()),  # Threshold line across all years\n",
    "    name=\"Threshold\",\n",
    "    line=dict(color='#333333', width=3),\n",
    "    mode='lines',\n",
    "    hovertemplate='Threshold:<br>%{y:.2f} ppm<extra></extra>'\n",
    "))\n",
    "\n",
    "# Calculate the trendline for all data combined (across all sites)\n",
    "slope, intercept, r_value, p_value, std_err = linregress(combined_df['Year'], combined_df['3_Year_Average'])\n",
    "\n",
    "# Create the trendline values for the whole dataset\n",
    "trendline_y = slope * combined_df['Year'] + intercept\n",
    "\n",
    "# Add the trendline to the figure\n",
    "fig_per_site.add_trace(go.Scatter(\n",
    "    x=combined_df['Year'],\n",
    "    y=trendline_y,\n",
    "    name=\"Global Trendline\",\n",
    "    line=dict(color='orange', width=3, dash='dash'),\n",
    "    hovertemplate=f'Global Trendline:<br>Slope {slope:.2f}<extra></extra>'\n",
    "))\n",
    "\n",
    "# Display the chart\n",
    "fig_per_site.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Load the combined data\n",
    "combined_df = pd.read_csv('C:\\\\Users\\\\snewsome\\\\Documents\\\\Threshold\\\\pm10combined_3_year_average.csv')\n",
    "\n",
    "# Filter out \"All Sites\" to get only individual site data\n",
    "df = combined_df[combined_df['Site'] != 'All Sites']\n",
    "\n",
    "# Drop rows with NaN values in 'Year' or '3_Year_Average' columns\n",
    "df = df.dropna(subset=['Year', '3_Year_Average'])\n",
    "\n",
    "# Setup plot\n",
    "fig = px.scatter(df, x='Year', y='3_Year_Average', color='Site')\n",
    "\n",
    "# Update hover template\n",
    "fig.update_traces(hovertemplate='<b>%{y:.2f}</b> ppm')\n",
    "\n",
    "# Create threshold line\n",
    "threshold_value = 20\n",
    "fig.add_trace(go.Scatter(\n",
    "    y=[threshold_value] * len(df),\n",
    "    x=df['Year'],\n",
    "    name=\"Threshold\",\n",
    "    line=dict(color='#333333', width=3),\n",
    "    mode='lines',\n",
    "    hovertemplate='Threshold: %{y:.0f} ppm<extra></extra>'\n",
    "))\n",
    "\n",
    "# Create trendline\n",
    "fig2 = px.scatter(df, x='Year', y='3_Year_Average', trendline='ols', trendline_color_override='#8a7121')\n",
    "\n",
    "# Set up trendline trace\n",
    "trendline = fig2.data[1]\n",
    "\n",
    "# Get ols results\n",
    "fit_results = px.get_trendline_results(fig2).px_fit_results.iloc[0]\n",
    "# Get beta value\n",
    "beta = fit_results.params[1]\n",
    "\n",
    "# Update trendline\n",
    "trendline.update(showlegend=True, name=\"Trend\", line_width=3, \n",
    "                 customdata=[beta] * len(trendline.y), hovertemplate='Trend: %{customdata:.2f}<extra></extra>')\n",
    "\n",
    "# Add to figure\n",
    "fig.add_trace(trendline)\n",
    "\n",
    "# Set layout\n",
    "fig.update_layout(\n",
    "    title=\"PM 10 - Annual - 3-Year Rolling Average\",\n",
    "    font_family='Arial',\n",
    "    template=template,\n",
    "    showlegend=True,\n",
    "    hovermode=\"x unified\",\n",
    "    xaxis=dict(\n",
    "        tickmode='linear',\n",
    "        tick0=1985,\n",
    "        dtick=5\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        tickmode='linear',\n",
    "        tick0=0,\n",
    "        dtick=5,\n",
    "        range=[0, 30],\n",
    "        title_text='Parts Per Million'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show figure\n",
    "fig.show()\n",
    "# save to HTML\n",
    "fig.write_html(os.path.join(draftworkspace, \"AirQuality_AnnualPM10_3YrAvg.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AQ Exceedance Analysis PM10 and PM 2.5 24HR High with exemption of Fires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------#\n",
    "#Import and Format Data from DRI\n",
    "#------------------------#\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "\n",
    "# 1. Specify File Paths\n",
    "file_paths = [r\"F:\\\\Research and Analysis\\\\Air Quality\\\\Annual Reports DRI\\\\AQ data 2023.xlsx\",\n",
    "              r\"F:\\\\Research and Analysis\\\\Air Quality\\\\Annual Reports DRI\\\\AQ data 2022.xlsx\",\n",
    "              r\"F:\\\\Research and Analysis\\\\Air Quality\\\\Annual Reports DRI\\\\AQ data 2021.xlsx\",\n",
    "              r\"F:\\\\Research and Analysis\\\\Air Quality\\\\Annual Reports DRI\\\\AQ data 2020.xlsx\",\n",
    "              r\"F:\\\\Research and Analysis\\\\Air Quality\\\\Annual Reports DRI\\\\AQ data 2019.xlsx\",\n",
    "              r\"F:\\Research and Analysis\\Air Quality\\Annual Reports DRI\\Daily AQ data 2018 from AQ'd data 2018.xlsx\"]\n",
    "\n",
    "# 2. Read Data from Each File Daily Data\n",
    "dfs = []  # List to store DataFrames from each file\n",
    "sheet_name = 'daily'  # Name of the sheet to read\n",
    "\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name, header=[0, 1])\n",
    "    dfs.append(df)\n",
    "\n",
    "# 3. Concatenate DAirrames\n",
    "DailyAir_df= pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Get rid of blank columnAirExcel\n",
    "DailyAir_df = DailyAir_df.dropna(axis=1, how='all')\n",
    "\n",
    "# Combine the first two rows to create a singAireader\n",
    "DailyAir_df.columns = [' '.join(col).strip() for col in DailyAir_df.columns.values]\n",
    "\n",
    "# Ensure the date column is correctly parsed\n",
    "DailyAir_df['date'] = pd.to_datetime(DailyAir_df['SITE date'], errors='coerce')\n",
    "\n",
    "\n",
    "# Keep only columns related to PM10 and PM2.5\n",
    "keep_columns = [col for col in DailyAir_df.columns if 'PM10' in col or 'O3max' in col or 'PM2.5' in col or 'date' in col]\n",
    "\n",
    "DailyAir_df = DailyAir_df[keep_columns]\n",
    "# Convert PM10 and PM2.5 columns to numeric, forcing errors to NaN\n",
    "DailyAir_df['SLT PM10avg'] = pd.to_numeric(DailyAir_df['SLT PM10avg'], errors='coerce')\n",
    "DailyAir_df['TC PM2.5avg'] = pd.to_numeric(DailyAir_df['TC PM2.5avg'], errors='coerce')\n",
    "DailyAir_df['STL PM2.5avg'] = pd.to_numeric(DailyAir_df['STL PM2.5avg'], errors='coerce')\n",
    "DailyAir_df['TC O3max'] = pd.to_numeric(DailyAir_df['TC O3max'], errors='coerce')\n",
    "DailyAir_df['STL O3max'] = pd.to_numeric(DailyAir_df['STL O3max'], errors='coerce')\n",
    "DailyAir_df['IV O3max'] = pd.to_numeric(DailyAir_df['IV O3max'], errors='coerce')\n",
    "DailyAir_df['BSP O3max'] = pd.to_numeric(DailyAir_df['BSP O3max'], errors='coerce')\n",
    "DailyAir_df['LTC O3max'] = pd.to_numeric(DailyAir_df['LTC O3max'], errors='coerce')\n",
    "\n",
    "#keep data from 2018-2023 --- Excel Table used has 2013-2018 data, before 2018 soo 2017 and earlier the data for STL PM2.5avg is actually PM10avg\n",
    "#DailyAir_df = DailyAir_df.loc[DailyAir_df['date'].dt.year >= 2018]\n",
    "\n",
    "# #Get rid of the following dates in the data because of fire exemptions\n",
    "# Define the date ranges to drop\n",
    "date_ranges_to_drop = [\n",
    "    #Loyalton Fire\n",
    "    pd.date_range(start='2020-08-19', end='2020-08-25'),\n",
    "    pd.date_range(start='2020-08-28', end='2020-08-31'),\n",
    "    #Creek Fire\n",
    "    pd.date_range(start='2020-09-08', end='2020-09-08'),\n",
    "    pd.date_range(start='2020-09-10', end='2020-09-18'),\n",
    "    pd.date_range(start='2020-09-21', end='2020-09-21'),\n",
    "    #August Complex Fire\n",
    "    pd.date_range(start='2020-09-30', end='2020-09-30'),\n",
    "    #Tamarack Fire\n",
    "    pd.date_range(start='2021-07-18', end='2021-07-18'),\n",
    "    #Tamarack Fire 9/23/2021, Tamarack and Dixie 9/24/2021\n",
    "    pd.date_range(start='2021-07-23', end='2021-07-24'),\n",
    "    #Tamarack and Dixie\n",
    "    pd.date_range(start='2021-09-26', end='2021-09-26'),\n",
    "    #Dixie Fire\n",
    "    pd.date_range(start='2021-08-03', end='2021-08-08'),\n",
    "    pd.date_range(start='2021-08-10', end='2021-08-10'),\n",
    "    pd.date_range(start='2021-08-12', end='2021-08-13'),\n",
    "    #Caldor Fire\n",
    "    pd.date_range(start='2021-08-14', end='2021-09-09'),\n",
    "    pd.date_range(start='2021-09-22', end='2021-09-22'),\n",
    "    pd.date_range(start='2021-09-25', end='2021-09-26'),\n",
    "    #Oak Fire\n",
    "    pd.date_range(start='2022-07-24', end='2022-07-24'),\n",
    "    #Mosquito Fire--check these dates\n",
    "    pd.date_range(start='2022-09-08', end= '2022-09-13'),\n",
    "    pd.date_range(start='2022-09-15', end= '2022-09-16'),\n",
    "]\n",
    "#MAke a new dataframe that grabs the maximumvalue for each year of data\n",
    "# Combine all dates to drop into a single list\n",
    "dates_to_drop = pd.concat([pd.Series(date_range) for date_range in date_ranges_to_drop])\n",
    "# Create a list of tuples with date ranges and corresponding fire names\n",
    "fire_dates = [\n",
    "    ('Loyalton Fire', pd.date_range(start='2020-08-19', end='2020-08-25')),\n",
    "    ('Loyalton Fire', pd.date_range(start='2020-08-28', end='2020-08-31')),\n",
    "    ('Creek Fire', pd.date_range(start='2020-09-08', end='2020-09-08')),\n",
    "    ('Creek Fire', pd.date_range(start='2020-09-10', end='2020-09-18')),\n",
    "    ('Creek Fire', pd.date_range(start='2020-09-21', end='2020-09-21')),\n",
    "    ('August Complex Fire', pd.date_range(start='2020-09-30', end='2020-09-30')),\n",
    "    ('Tamarack Fire', pd.date_range(start='2021-07-18', end='2021-07-18')),\n",
    "    ('Tamarack Fire', pd.date_range(start='2021-07-23', end='2021-07-23')),\n",
    "    ('Tamarack and Dixie', pd.date_range(start='2021-07-24', end='2021-07-24')),\n",
    "    ('Tamarack and Dixie', pd.date_range(start='2021-07-26', end='2021-07-26')),\n",
    "    ('Dixie Fire', pd.date_range(start='2021-08-03', end='2021-08-08')),\n",
    "    ('Dixie Fire', pd.date_range(start='2021-08-10', end='2021-08-10')),\n",
    "    ('Dixie Fire', pd.date_range(start='2021-08-12', end='2021-08-13')),\n",
    "    ('Caldor Fire', pd.date_range(start='2021-08-14', end='2021-09-09')),\n",
    "    ('Caldor Fire', pd.date_range(start='2021-09-22', end='2021-09-22')),\n",
    "    ('Caldor Fire', pd.date_range(start='2021-09-25', end='2021-09-26')),\n",
    "    ('Oak Fire', pd.date_range(start='2022-07-24', end='2022-07-24')),\n",
    "    ('Mosquito Fire', pd.date_range(start='2022-09-08', end='2022-09-13')),\n",
    "    ('Mosquito Fire', pd.date_range(start='2022-09-15', end='2022-09-16')),\n",
    "]\n",
    "\n",
    "# Flatten the list of tuples into a DataFrame\n",
    "fire_dates_flat = [(date, fire) for fire, dates in fire_dates for date in dates]\n",
    "Fire_dates = pd.DataFrame(fire_dates_flat, columns=['date', 'fire'])\n",
    "# Add Threshold Value for each indicator  PM10 and PM2.5\n",
    "\n",
    "# Filter out the dates from the DataFrame\n",
    "DailyExemptdf = DailyAir_df[~DailyAir_df['date'].isin(dates_to_drop)]\n",
    "\n",
    "#Create Year column for each dataframe one with all dates and one with fire exempted dates\n",
    "DailyExemptdf['Year'] = DailyExemptdf['date'].dt.year\n",
    "DailyAir_df['Year'] = DailyAir_df['date'].dt.year\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### PM 10 ###########\n",
    "#---------------------------------------\n",
    "#Calculate Exceedances/Days that went over Threshold Value CA and NAAQS#\n",
    "#---------------------------------------\n",
    "# Get a list of all years from the data\n",
    "all_years = DailyExemptdf['Year'].unique()\n",
    "\n",
    "# Calculate the exceedances for CA threshold -excluded fire dates\n",
    "PM10_exceedances_CA_exempt = DailyExemptdf[DailyExemptdf['SLT PM10avg'] > 50].groupby('Year').size().reset_index(name='Exceedances_CA')\n",
    "PM10_exceedances_CA_exempt = PM10_exceedances_CA_exempt.set_index('Year').reindex(all_years, fill_value=0).reset_index()\n",
    "\n",
    "# Calculate the exceedances for NAAQS threshold- excluded fire dates\n",
    "PM10_exceedances_NAAQS_exempt = DailyExemptdf[DailyExemptdf['SLT PM10avg'] > 150].groupby('Year').size().reset_index(name='Exceedances_NAAQS')\n",
    "PM10_exceedances_NAAQS_exempt = PM10_exceedances_NAAQS_exempt.set_index('Year').reindex(all_years, fill_value=0).reset_index()\n",
    "\n",
    "# Calculate the exceedances for CA threshold (all data)\n",
    "PM10_exceedances_CA_all = DailyAir_df[DailyAir_df['SLT PM10avg'] > 50].groupby('Year').size().reset_index(name='Exceedances_CA')\n",
    "PM10_exceedances_CA_all = PM10_exceedances_CA_all.set_index('Year').reindex(all_years, fill_value=0).reset_index()\n",
    "\n",
    "# Calculate the exceedances for NAAQS threshold (all data)\n",
    "PM10_exceedances_NAAQS_all = DailyAir_df[DailyAir_df['SLT PM10avg'] > 150].groupby('Year').size().reset_index(name='Exceedances_NAAQS')\n",
    "PM10_exceedances_NAAQS_all = PM10_exceedances_NAAQS_all.set_index('Year').reindex(all_years, fill_value=0).reset_index()\n",
    "\n",
    "#Join all exceedance data into one dataframe\n",
    "PM10_exceedances_CA = pd.merge(PM10_exceedances_CA_exempt, PM10_exceedances_CA_all, on='Year', suffixes=('_exempt', '_all'))\n",
    "PM10_excedances_NAAQS = pd.merge(PM10_exceedances_NAAQS_exempt, PM10_exceedances_NAAQS_all, on='Year', suffixes=('_exempt', '_all'))\n",
    "\n",
    "PM10_exceedances= pd.merge(PM10_exceedances_CA, PM10_excedances_NAAQS, on='Year')\n",
    "#----------------------------------------------\n",
    "# Calculate the maximum value per year for PM10\n",
    "#----------------------------------------------\n",
    "PM10max_values_exempt = DailyExemptdf.groupby('Year').agg({'SLT PM10avg': 'max'}).reset_index()\n",
    "PM10max_values_all = DailyAir_df.groupby('Year').agg({'SLT PM10avg': 'max'}).reset_index()    \n",
    "\n",
    "# Rename columns for clarity\n",
    "PM10max_values_exempt.rename(columns={'SLT PM10avg': 'Max_PM10'}, inplace=True)\n",
    "PM10max_values_all.rename(columns={'SLT PM10avg': 'Max_PM10'}, inplace=True)\n",
    "\n",
    "#Add Threshold Value too dataframe\n",
    "PM10max_values_exempt['Threshold Value CA'] = 50\n",
    "PM10max_values_exempt['Threshold Value NAAQS'] = 150\n",
    "PM10max_values_all['Threshold Value CA'] = 50\n",
    "PM10max_values_all['Threshold Value NAAQS'] = 150\n",
    "print('With Fire Exemption Dates:')\n",
    "print(PM10max_values_exempt)\n",
    "print('All Dates:')\n",
    "print(PM10max_values_all)\n",
    "\n",
    "#----------------------------------------------\n",
    "# Combine PM10 data into one DataFrame\n",
    "#----------------------------------------------\n",
    "PM10max_combined = pd.merge(PM10max_values_exempt, PM10max_values_all, on='Year', suffixes=('_exempt', '_all'))\n",
    "\n",
    "# Add the necessary threshold values\n",
    "PM10max_combined['Threshold Value CA'] = 50\n",
    "PM10max_combined['Threshold Value NAAQS'] = 150\n",
    "PM10_exceedances['Exceedance_Threshold'] = \"1 exceedance per year on average over 3 years\"\n",
    "\n",
    "PM10_combined = pd.merge(PM10max_combined, PM10_exceedances, on='Year')\n",
    "\n",
    "# Keep only the necessary columns\n",
    "PM10_combined = PM10_combined[['Year', 'Exceedances_CA_exempt', 'Exceedances_CA_all', 'Exceedances_NAAQS_exempt','Exceedances_NAAQS_all','Exceedance_Threshold', 'Max_PM10_exempt', 'Max_PM10_all', 'Threshold Value CA', 'Threshold Value NAAQS']]\n",
    "\n",
    "print('Combined PM10 Data:')\n",
    "print(PM10_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######### PM 2.5 ###########\n",
    "#----------------------------------------------\n",
    "#CALCULATE 3 year average 98th percentil \n",
    "# Calculate the 98th percentile for each year for both TC PM2.5avg and STL PM2.5avg\n",
    "PM25_98th_percentile_exempt = DailyExemptdf.groupby('Year').agg({'TC PM2.5avg': lambda x: x.quantile(0.98)}).reset_index()\n",
    "PM25_98th_percentile_all = DailyAir_df.groupby('Year').agg({'TC PM2.5avg': lambda x: x.quantile(0.98)}).reset_index()\n",
    "\n",
    "\n",
    "# Rename columns for clarity\n",
    "PM25_98th_percentile_exempt.rename(columns={'TC PM2.5avg': '98th_Percentile_PM2.5'}, inplace=True)\n",
    "PM25_98th_percentile_all.rename(columns={'TC PM2.5avg': '98th_Percentile_PM2.5'}, inplace=True)\n",
    "\n",
    "# Calculate the 3-year rolling mean of the 98th percentile values\n",
    "PM25_98th_percentile_exempt['3_Year_Mean_98th_Percentile'] = PM25_98th_percentile_exempt['98th_Percentile_PM2.5'].rolling(window=3).mean()\n",
    "PM25_98th_percentile_all['3_Year_Mean_98th_Percentile'] = PM25_98th_percentile_all['98th_Percentile_PM2.5'].rolling(window=3).mean()\n",
    "#print('With Fire Exemptions 98 percentile 3 year mean:')\n",
    "#print(PM25_98th_percentile_exempt)\n",
    "#print('All Dates 98 percentile 3 year mean:')\n",
    "#print(PM25_98th_percentile_all)\n",
    "\n",
    "#-----------------------------------------\n",
    "# Calculate Exceedances/Days that went over Threshold Value CA and NAAQS with and without fire exempt days\n",
    "#-----------------------------------------\n",
    "# Get a list of all years from the data\n",
    "all_years = DailyExemptdf['Year'].unique()\n",
    "\n",
    "#98% values used\n",
    "# Calculate the exceedances for NAAQS threshold- excluded fire dates using 98th percentile of values\n",
    "PM25_98_exceedances_NAAQS_exempt = PM25_98th_percentile_exempt[PM25_98th_percentile_exempt['98th_Percentile_PM2.5'] > 35].groupby('Year').size().reset_index(name='98Exceedances_NAAQS')\n",
    "PM25_98_exceedances_NAAQS_exempt = PM25_98_exceedances_NAAQS_exempt.set_index('Year').reindex(all_years, fill_value=0).reset_index()\n",
    "\n",
    "# Calculate the exceedances for NAAQS threshold (all data)\n",
    "PM25_98_exceedances_NAAQS_all = PM25_98th_percentile_all[PM25_98th_percentile_all['98th_Percentile_PM2.5'] > 35].groupby('Year').size().reset_index(name='98Exceedances_NAAQS')\n",
    "PM25_98_exceedances_NAAQS_all = PM25_98_exceedances_NAAQS_all.set_index('Year').reindex(all_years, fill_value=0).reset_index()\n",
    "\n",
    "#Raw Data Used\n",
    "# Calculate the exceedances for NAAQS threshold- excluded fire dates \n",
    "PM25_exceedances_NAAQS_exempt = DailyExemptdf[DailyExemptdf['TC PM2.5avg'] > 35].groupby('Year').size().reset_index(name='Exceedances_NAAQS')\n",
    "PM25_exceedances_NAAQS_exempt = PM25_exceedances_NAAQS_exempt.set_index('Year').reindex(all_years, fill_value=0).reset_index()\n",
    "\n",
    "# Calculate the exceedances for NAAQS threshold (all data)\n",
    "PM25_exceedances_NAAQS_all = DailyAir_df[DailyAir_df['TC PM2.5avg'] > 35].groupby('Year').size().reset_index(name='Exceedances_NAAQS')\n",
    "PM25_exceedances_NAAQS_all = PM25_exceedances_NAAQS_all.set_index('Year').reindex(all_years, fill_value=0).reset_index()\n",
    "#Join all exceedance data into one dataframe\n",
    "PM25_98_exceedances = pd.merge(PM25_98_exceedances_NAAQS_exempt, PM25_98_exceedances_NAAQS_all, on='Year', suffixes=('_exempt', '_all'))\n",
    "PM25_raw_exceedances = pd.merge(PM25_exceedances_NAAQS_exempt, PM25_exceedances_NAAQS_all, on='Year', suffixes=('_exempt', '_all'))\n",
    "PM25_exceedances = pd.merge(PM25_98_exceedances, PM25_raw_exceedances, on='Year')\n",
    "\n",
    "\n",
    "#----------------------------------------------\n",
    "#CALCULATE MAX ANNUAL VALUES FOR PM 2.5\n",
    "#----------------------------------------------\n",
    "#Calculate the maximym annual value of average daily values\n",
    "PM25max_values_exempt = DailyExemptdf.groupby('Year').agg({'TC PM2.5avg': 'max'}).reset_index()\n",
    "\n",
    "PM25max_values_all = DailyAir_df.groupby('Year').agg({'TC PM2.5avg': 'max'}).reset_index()\n",
    "# Rename columns for clarity\n",
    "PM25max_values_exempt.rename(columns={'TC PM2.5avg': 'Max_PM2.5'}, inplace=True)\n",
    "PM25max_values_all.rename(columns={'TC PM2.5avg': 'Max_PM2.5'}, inplace=True)\n",
    "#Add Threshold Value too dataframe\n",
    "PM25max_values_exempt['Threshold Value CA'] = 'None'\n",
    "PM25max_values_exempt['Threshold Value NAAQS'] = 35\n",
    "PM25max_values_all['Threshold Value CA'] = 'None'\n",
    "PM25max_values_all['Threshold Value NAAQS'] = 35\n",
    "#print('With Fire Exemption Dates:')\n",
    "#print(PM25max_values_exempt)\n",
    "#print('All Dates:')\n",
    "#print(PM25max_values_all)\n",
    "# Combine PM25 data into one DataFrame\n",
    "PM25max_combined = pd.merge(PM25max_values_exempt, PM25max_values_all, on='Year', suffixes=('_exempt', '_all'))\n",
    "PM2598_combined = pd.merge(PM25_98th_percentile_exempt, PM25_98th_percentile_all, on='Year', suffixes=('_exempt', '_all'))\n",
    "# Merging the first two DataFrames\n",
    "# List of DataFrames to merge\n",
    "dfs = [PM25max_combined, PM2598_combined, PM25_exceedances]\n",
    "\n",
    "from functools import reduce\n",
    "# Merge all DataFrames on 'Year'\n",
    "PM25_combined = reduce(lambda left, right: pd.merge(left, right, on='Year'), dfs)\n",
    "\n",
    "#PM25_partial_merge = pd.merge(PM25max_combined, PM2598_combined, on='Year')\n",
    "\n",
    "# Merging the result with the third DataFrame\n",
    "#PM25_combined = pd.merge(PM25_partial_merge, PM25_exceedances, on='Year')\n",
    "# Keep only the necessary threshold values\n",
    "PM25_combined = PM25_combined[['Year', 'Max_PM2.5_exempt','Max_PM2.5_all', '3_Year_Mean_98th_Percentile_exempt',  \n",
    "                          '3_Year_Mean_98th_Percentile_all', '98th_Percentile_PM2.5_exempt', '98th_Percentile_PM2.5_all', 'Exceedances_NAAQS_exempt',\n",
    "                           'Exceedances_NAAQS_all', '98Exceedances_NAAQS_exempt', '98Exceedances_NAAQS_all',\n",
    "                           'Threshold Value CA_exempt', 'Threshold Value NAAQS_exempt']]\n",
    "\n",
    "# Rename columns for clarity\n",
    "PM25_combined.rename(columns={'Threshold Value CA_exempt': 'Threshold Value CA', \n",
    "                            'Threshold Value NAAQS_exempt': 'Threshold Value NAAQS'}, inplace=True)\n",
    "\n",
    "print('Combined pm2.5 Data:')\n",
    "print(PM25_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######O3 -1HR AVG######\n",
    "#Calculate Exceedances/Days that went over Threshold Value CA and NAAQS\n",
    "# Get a list of all years from the data\n",
    "all_years = DailyExemptdf['Year'].unique()\n",
    "# Calculate the exceedances for CA threshold -excluded fire dates\n",
    "O3_exceedances_CA_exempt = DailyExemptdf[(DailyExemptdf[['TC O3max', 'STL O3max', 'IV O3max']] > 90).any(axis=1)].groupby('Year').size().reset_index(name='Exceedances_CA')\n",
    "O3_exceedances_CA_exempt = O3_exceedances_CA_exempt.set_index('Year').reindex(all_years, fill_value=0).reset_index()\n",
    "\n",
    "# Calculate the exceedances for CA threshold (all data)\n",
    "O3_exceedances_CA_all = DailyAir_df[(DailyAir_df[['TC O3max', 'STL O3max', 'IV O3max']] > 90).any(axis=1)].groupby('Year').size().reset_index(name='Exceedances_CA')\n",
    "O3_exceedances_CA_all = O3_exceedances_CA_all.set_index('Year').reindex(all_years, fill_value=0).reset_index()\n",
    "\n",
    "# Calculate the exceedances for TRPA threshold -excluded fire dates\n",
    "O3_exceedances_TRPA_exempt = DailyExemptdf[(DailyExemptdf[['TC O3max', 'STL O3max', 'IV O3max']] > 80).any(axis=1)].groupby('Year').size().reset_index(name='Exceedances_TRPA')\n",
    "O3_exceedances_TRPA_exempt = O3_exceedances_TRPA_exempt.set_index('Year').reindex(all_years, fill_value=0).reset_index()\n",
    "\n",
    "# Calculate the exceedances for TRPA threshold (all data)\n",
    "O3_exceedances_TRPA_all = DailyAir_df[(DailyAir_df[['TC O3max', 'STL O3max', 'IV O3max']] > 80).any(axis=1)].groupby('Year').size().reset_index(name='Exceedances_TRPA')\n",
    "O3_exceedances_TRPA_all = O3_exceedances_TRPA_all.set_index('Year').reindex(all_years, fill_value=0).reset_index()\n",
    "\n",
    "#Join all exceedance data into one dataframe\n",
    "O3_exceedances_CA = pd.merge(O3_exceedances_CA_exempt, O3_exceedances_CA_all, on='Year', suffixes=('_exempt', '_all'))\n",
    "O3_excedances_TRPA = pd.merge(O3_exceedances_TRPA_exempt, O3_exceedances_TRPA_all, on='Year', suffixes=('_exempt', '_all'))\n",
    "\n",
    "O3_exceedances= pd.merge(O3_exceedances_CA, O3_excedances_TRPA, on='Year')\n",
    "\n",
    "\n",
    "# Calculate the maximum value per year for O3\n",
    "O3max_values_exempt = DailyExemptdf.groupby('Year').agg({'TC O3max': 'max', 'STL O3max': 'max', 'IV O3max':'max'}).reset_index()\n",
    "O3max_values_all = DailyAir_df.groupby('Year').agg({'TC O3max': 'max', 'STL O3max': 'max', 'IV O3max':'max'}).reset_index()    \n",
    "\n",
    "#Add Threshold Value too dataframe- values used in Air Quality Data 90 instead of 0.09\n",
    "O3max_values_exempt['O3Threshold Value CA'] = 90\n",
    "O3max_values_exempt['O3Threshold Value NAAQS'] = 'None'\n",
    "O3max_values_all['Threshold Value CA'] = 90\n",
    "O3max_values_all['Threshold Value NAAQS'] = 'None'\n",
    "O3max_values_exempt['Threshold Value TRPA'] = 80\n",
    "O3max_values_all['Threshold Value TRPA'] = 80\n",
    "print('With Fire Exemption Dates:')\n",
    "print(O3max_values_exempt)\n",
    "print('All Dates:')\n",
    "print(O3max_values_all)\n",
    "# Combine O3 data into one DataFrame\n",
    "O3max_combined = pd.merge(O3max_values_exempt, O3max_values_all, on='Year', suffixes=('_exempt', '_all'))\n",
    "O3_combined = pd.merge(O3max_combined, O3_exceedances, on='Year')\n",
    "# Keep only the necessary threshold values\n",
    "O3_combined = O3_combined[['Year', 'Exceedances_TRPA_all', 'Exceedances_TRPA_exempt', 'Exceedances_CA_all', \n",
    "                           'Exceedances_CA_exempt', 'TC O3max_exempt', 'TC O3max_all','STL O3max_exempt', \n",
    "                            'STL O3max_all', 'IV O3max_all', 'IV O3max_exempt',\n",
    "                           'Threshold Value CA', 'Threshold Value NAAQS', 'Threshold Value TRPA_all']]\n",
    "\n",
    "# Rename columns for clarity\n",
    "O3_combined.rename(columns={'Threshold Value CA_exempt': 'Threshold Value CA', \n",
    "                            'Threshold Value NAAQS_exempt': 'Threshold Value NAAQS', \n",
    "                            'Threshold Value TRPA_all': 'Threshold Value TRPA'}, inplace=True)\n",
    "\n",
    "print('Combined O3 Data:')\n",
    "print(O3_combined)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create Excel Doc with a sheet for each of the indicators: 2.5 combined, pm10 combine, and 03 combined\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "local_path= r'C:\\Users\\snewsome\\Documents\\Threshold\\AQFireexemptionsexceedances18-2023.xlsx'\n",
    "with ExcelWriter(local_path) as writer:\n",
    "    # Write each dataframe to a different worksheet.\n",
    "    PM10_combined.to_excel(writer, sheet_name='PM10', index=False)\n",
    "    PM25_combined.to_excel(writer, sheet_name='PM25',index=False)\n",
    "    O3_combined.to_excel(writer, sheet_name='O3', index=False)\n",
    "    Fire_dates.to_excel(writer, sheet_name='Fire Dates', index=False)\n",
    "\n",
    "\n",
    "# combine all dataframes \n",
    "#Final_df = pd.concat([PM10max_values_exempt.set_index('Year'), \n",
    " #                     PM10max_values_all.set_index('Year'), \n",
    "  #                    PM25_98th_percentile_exempt.set_index('Year'), \n",
    "   #                   PM25_98th_percentile_all.set_index('Year'), \n",
    "    #                  PM25max_values_exempt.set_index('Year'), \n",
    "     #                 PM25max_values_all.set_index('Year'), \n",
    "      #                O3max_values_exempt.set_index('Year'), \n",
    "       #               O3max_values_all.set_index('Year')], axis=1).reset_index()  \n",
    "#Write final dataframe with all data to excel\n",
    "#Final_df.to_excel('F:\\Research and Analysis\\Air Quality\\Annual Reports DRI\\AQdataFireexemptions2023TE.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Air Quality Data to table in Arc Pro--GDB created is to be appended manually until fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------#\n",
    "#Add Data to SDE\n",
    "#------------------------#\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Specify File Paths\n",
    "file_paths = [\"F:\\Research and Analysis\\Air Quality\\Annual Reports DRI\\AQ data 2023.xlsx\",\n",
    "              \"F:\\Research and Analysis\\Air Quality\\Annual Reports DRI\\AQ data 2022.xlsx\",\n",
    "              \"F:\\Research and Analysis\\Air Quality\\Annual Reports DRI\\AQ data 2021.xlsx\",\n",
    "              \"F:\\Research and Analysis\\Air Quality\\Annual Reports DRI\\AQ data 2020.xlsx\"]\n",
    "\n",
    "# 2. Read Data from Each File\n",
    "dfs = []  # List to store DataFrames from each file\n",
    "sheet_name = 'Indicator Values'  # Name of the sheet to read\n",
    "\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    dfs.append(df)\n",
    "\n",
    "# 3. Concatenate DataFrames\n",
    "Airdata_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "#Create layer in arcgis pro to be appended to sde.ThresholdEvaluyation_AirQuality\n",
    "\n",
    "#Fix up field names\n",
    "\n",
    "# Rename fields\n",
    "Airdata_df.rename(columns={'Data?': 'Data', 'Data Source': 'Data_Source'}, inplace=True)\n",
    "#Add columsn to dataframe\n",
    "Airdata_df['Threshold_Value']= None\n",
    "Airdata_df['Percent_of_Threshold_Value']= None\n",
    "Airdata_df['Include_in_Trend_Analysis']= None\n",
    "print(Airdata_df)\n",
    "\n",
    "\n",
    "# Improve code next year , ADD DLBLISS as yes for pm2.5 high24, south lake tahoe sites for pm2.5 avg. etc.\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = Airdata_df.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(Airdata_df.columns)\n",
    "\n",
    "\n",
    "# Create an empty table in the workspace\n",
    "table_path = os.path.join(workspace, 'Air_staging2')\n",
    "arcpy.CreateTable_management(workspace, 'Air_staging2')\n",
    "\n",
    "# Define the fields to include in the table with specified field types\n",
    "field_types = {'Indicator': 'Text', 'Pollutant': 'Text', 'Statistic': 'Text', 'Year': 'Long', 'Data': 'Text', 'Site': 'Text', 'Value': 'Double', 'Data_Source': 'Text', 'Exceedances': 'Long', 'Threshold_Value': 'Text', 'Percent_of_Threshold_Value': 'Text', 'Include_in_Trend_Analysis': 'Text'}\n",
    "\n",
    "# Add fields to the table with specified field types\n",
    "for field_name, field_type in field_types.items():\n",
    "    arcpy.AddField_management(table_path, field_name, field_type)\n",
    "\n",
    "# Convert the DataFrame to a structured numpy array\n",
    "array = Airdata_df[list(field_types.keys())].to_records(index=False)\n",
    "\n",
    "# Insert data into the table\n",
    "with arcpy.da.InsertCursor(table_path, list(field_types.keys())) as cursor:\n",
    "    for row in array:\n",
    "        cursor.insertRow(row)\n",
    "\n",
    "print(\"Data table successfully made.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearshore Periphyton/Algae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set colors\n",
    "color_discrete_map = {'Incline West': '#33a02c',\n",
    "                      'Pineland': '#1f78b4', \n",
    "                      'Rubicon': '#fb9a99',\n",
    "                      'Sugarpine': '#911eb4',\n",
    "                      'Tahoe City': '#ff7f00',\n",
    "                      'Zephyr': '#cab2d6'                                \n",
    "                        }\n",
    "#Import DAta\n",
    "file_path = r\"F:\\Research and Analysis\\Water Quality Monitoring Program\\Nearshore\\IntegratedAlgaeMonitoring\\data\\Peri\"\n",
    "\n",
    "# Load each CSV file into a DataFrame\n",
    "inclinedf = pd.read_csv(os.path.join(file_path, 'InclineWest_Historic.csv'))\n",
    "Pinelanddf = pd.read_csv(os.path.join(file_path, 'Pineland_Historic.csv'))\n",
    "Rubicondf = pd.read_csv(os.path.join(file_path, 'Rubicon_Historic.csv'))\n",
    "Sugarpinedf = pd.read_csv(os.path.join(file_path, 'Sugarpine_Historic.csv'))\n",
    "TahoeCitydf = pd.read_csv(os.path.join(file_path, 'TahoeCity_Historic.csv'))\n",
    "Zephyrdf = pd.read_csv(os.path.join(file_path, 'Zephyr_Historic.csv'))\n",
    "\n",
    "#Combine All Dataframes\n",
    "combined_df = pd.concat([inclinedf, Pinelanddf, Rubicondf, Sugarpinedf, TahoeCitydf, Zephyrdf], ignore_index=True)\n",
    "# Ensure the Date column is in datetime format\n",
    "combined_df['Sample_Date'] = pd.to_datetime(combined_df['Sample_Date'])\n",
    "\n",
    "# Extract the year from the Date column and create a new Year column\n",
    "combined_df['Year'] = combined_df['Sample_Date'].dt.year\n",
    "\n",
    "#Group by 'Year' and 'Site', then calculate the average of the 'chl' column\n",
    "grouped_df = combined_df.groupby(['Year', 'site'], as_index=False)['Chl'].mean()\n",
    "\n",
    "# Preview the resulting DataFrame\n",
    "print(grouped_df.head())\n",
    "\n",
    "#May need to drop rows with missing value? not sure how it will show up on chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "color_discrete_map = {'Incline West': '#008080',\n",
    "                      'Pineland': '#FF6F61', \n",
    "                      'Rubicon Pt.': '#cab2d6',\n",
    "                      'Sugar Pine Pt.': '#4169E1',\n",
    "                      'Tahoe City': '#DAA520',\n",
    "                      'Zephyr Pt.': '#708090'                                \n",
    "                        }\n",
    "\n",
    "df = grouped_df\n",
    "\n",
    "\n",
    "# setup plot\n",
    "fig = px.line(df, x = 'Year', y= 'Chl', color='site',\n",
    "                 color_discrete_map = color_discrete_map)\n",
    "\n",
    "fig.update_traces(hovertemplate='<br>%{y:.2f}')\n",
    "\n",
    "\n",
    "# set layout\n",
    "fig.update_layout(title=\"Nearshore Attached Algae - Average Chlorophyll\",\n",
    "                    font_family=font,\n",
    "                    template=template,\n",
    "                    showlegend=True,\n",
    "                    hovermode=\"x unified\",\n",
    "                    xaxis = dict(\n",
    "                        tickmode = 'linear',\n",
    "                        tick0 = 1985,\n",
    "                        dtick = 5\n",
    "                    ),\n",
    "                    yaxis = dict(\n",
    "                        tickmode = 'linear',\n",
    "                        tick0 = 0,\n",
    "                        dtick = 5,\n",
    "                        range=[0, 110],\n",
    "                        title_text='Average Chl'\n",
    "                    )\n",
    "                  \n",
    "                 )\n",
    "\n",
    "# show figure\n",
    "fig.show()\n",
    "# save to HTML\n",
    "fig.write_html(os.path.join(workspace, \"WaterQuality_NearshoreChl.html\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
