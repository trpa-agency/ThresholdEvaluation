{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "from arcgis import GIS\n",
    "from arcgis.features import FeatureLayer\n",
    "from arcgis.features import GeoAccessor\n",
    "import os\n",
    "import numpy as np\n",
    "import arcpy\n",
    "from soil_conservation import *\n",
    "workspace =r'F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\n",
    "draftworkspace =r'C:\\Users\\snewsome\\Documents\\GitHub\\ThresholdEvaluation\\2023\\WaterQuality' \n",
    "# make sql database connection with pyodbc\n",
    "#conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER=sql12;DATABASE=sde_tabular;UID=sde;PWD=staff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soil Conservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfImpChg.GISAcre.sum()\n",
    "# group by Surface and sum\n",
    "df = dfImpChg.groupby('Surface')[\"GISAcre\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get land capability data\n",
    "dfImpChg, dfImp2019 = get_soil_conservation_data_web()\n",
    "\n",
    "df = dfImpChg.astype({\"Land_Capab\": str})\n",
    "df = df.rename(columns={'Land_Capab':'Land Capability', 'GISAcre':'Total Acres', 'OWNERSHIP_': 'Ownership'})\n",
    "df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "df = df[df['Land Capability'].notna()]\n",
    "df.set_index('Land Capability')\n",
    "dfLCType = df.groupby(\"Land Capability\")[\"Total Acres\"].sum().reset_index()\n",
    "\n",
    "df = dfLCType.replace(to_replace='None', value=np.nan).dropna()\n",
    "\n",
    "# 2019 analysis results\n",
    "df = dfImp2019.astype({\"Land_Capab\": str})\n",
    "df = df.rename(columns={'Land_Capab':'Land Capability', 'GISAcre':'Acre'})\n",
    "df.set_index('Land Capability')\n",
    "\n",
    "# trails were drawn to wide, reduce acreage by 50%\n",
    "df.loc[(df['Surface']=='Soft')&(df['Feature']=='Trail'), 'Acre'] = df.Acre * 0.5 \n",
    "\n",
    "# pivot land capbility by acres of surface type\n",
    "pivotSoilImp = pd.pivot_table(df,index=['Land Capability'],\n",
    "                            columns='Surface',\n",
    "                            values=['Acre'], \n",
    "                            aggfunc=np.sum,fill_value=0)\n",
    "\n",
    "flattened = pd.DataFrame(pivotSoilImp.to_records())\n",
    "\n",
    "df = flattened.rename(columns={\"('Acre', 'Hard')\":'Acres of Hard Surface 2019',\n",
    "                            \"('Acre', 'Soft')\":'Acres of Soft Surface 2019'})\n",
    "df\n",
    "# # replace all spaces ond blanks with NaN\n",
    "# df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "# df = df[df['Land Capability'].notna()]\n",
    "\n",
    "# # calculate acres of coverage\n",
    "# df[\"Acres of Coverage 2019\"]= df[\"Acres of Hard Surface 2019\"]+df[\"Acres of Soft Surface 2019\"]\n",
    "\n",
    "# # merge grouped land capability data frame with impervious pivot data frame\n",
    "# dfMerge = pd.merge(df, dfLCType, on='Land Capability')\n",
    "\n",
    "# # rename field\n",
    "# df = dfMerge.rename(columns={'Acre':'Total Acres'})\n",
    "\n",
    "# # calculate perent coverage\n",
    "# df['Percent Hard 2019'] = (df['Acres of Hard Surface 2019']/df['Total Acres'])*100\n",
    "# df['Percent Soft 2019'] = (df['Acres of Soft Surface 2019']/df['Total Acres'])*100\n",
    "# df['Percent Impervious 2019'] = ((df['Acres of Hard Surface 2019']+df['Acres of Soft Surface 2019'])/df['Total Acres'])*100\n",
    "\n",
    "# # record percent allowed field\n",
    "# df['Threshold Value'] = \"0%\"\n",
    "# df.loc[df['Land Capability'].isin(['1A','1B','1C','2']), 'Threshold Value'] = \"1%\"\n",
    "# df.loc[df['Land Capability'].isin(['3']), 'Threshold Value'] = \"5%\"\n",
    "# df.loc[df['Land Capability'].isin(['4']), 'Threshold Value'] = \"20%\"\n",
    "# df.loc[df['Land Capability'].isin(['5']), 'Threshold Value'] = \"25%\"\n",
    "# df.loc[df['Land Capability'].isin(['6','7']), 'Threshold Value'] = \"30%\"\n",
    "\n",
    "# # determine acres of coverage allowed per land capability\n",
    "# df['Threshold Acres'] = 0\n",
    "# df.loc[df['Land Capability'].isin(['1A','1B','1C','2']), 'Threshold Acres'] = df['Total Acres']*0.01\n",
    "# df.loc[df['Land Capability'].isin(['3']), 'Threshold Acres'] = df['Total Acres']*0.05\n",
    "# df.loc[df['Land Capability'].isin(['4']), 'Threshold Acres'] = df['Total Acres']*0.2\n",
    "# df.loc[df['Land Capability'].isin(['5']), 'Threshold Acres'] = df['Total Acres']*0.25\n",
    "# df.loc[df['Land Capability'].isin(['6','7']), 'Threshold Acres'] = df['Total Acres']*0.3\n",
    "\n",
    "# df2019 = df.drop(columns=['Threshold Acres', 'Threshold Value', 'Total Acres'])\n",
    "\n",
    "# df = df[['Land Capability',\n",
    "#         'Acres of Hard Surface 2019',\n",
    "#         'Acres of Soft Surface 2019',\n",
    "#         'Acres of Coverage 2019',\n",
    "#         'Percent Hard 2019',\n",
    "#         'Percent Soft 2019',\n",
    "#         'Percent Impervious 2019',\n",
    "#         'Total Acres',\n",
    "#         'Threshold Value',\n",
    "#         'Threshold Acres']]\n",
    "\n",
    "# df.dropna(subset=['Land Capability'], inplace=True)\n",
    "\n",
    "# df['Land Capability']= pd.Categorical(df['Land Capability'], ['1A', '1B', '1C', '2', '3', '4', '5', '6', '7'])\n",
    "\n",
    "# df.sort_values(by=\"Land Capability\")\n",
    "# df.set_index('Land Capability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNewImp = pd.read_csv(r\"data\\raw_data\\FinalCoverageChanges_2020-2023.csv\")\n",
    "\n",
    "# fill any NaN values with 0\n",
    "dfNewImp.fillna(0, inplace=True)\n",
    "# drop parcle and jurisdiction columns\n",
    "dfNewImp.drop(['Parcel', 'Jurisdiction'], axis=1, inplace=True)\n",
    "# group by\n",
    "df = dfNewImp.groupby(['Bailey1a', 'Bailey1b', 'Bailey1c', 'Bailey2', 'Bailey3', 'Bailey4', 'Bailey5', 'Bailey6', 'Bailey7']).sum()\n",
    "df = df.reset_index()\n",
    "# drop total column\n",
    "# df.drop('Total', axis=1, inplace=True)\n",
    "# stack the dataframe\n",
    "df = df.stack().reset_index()\n",
    "# # rename columns\n",
    "df.rename(columns={'level_1':'LandCapability', 0:'SqFt'}, inplace=True)\n",
    "# # drop columns\n",
    "df.drop(['level_0'], axis=1, inplace=True)\n",
    "# pivot the dataframe\n",
    "pivot = pd.pivot_table(df,index=['LandCapability'],\n",
    "                              values='SqFt', aggfunc=np.sum)\n",
    "# flatten pivot\n",
    "flattened = pd.DataFrame(pivot.to_records())\n",
    "# create acres column\n",
    "flattened['Acres'] = flattened['SqFt']/43560\n",
    "# to csv\n",
    "flattened.to_csv(r\"data/processed_data/LandCapability_Acres.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "#import pandas as pd\n",
    "#import os\n",
    "#from arcgis import GIS\n",
    "#from arcgis.features import FeatureLayer\n",
    "# Import geopandas\n",
    "#import geopandas as gpd\n",
    "#from shapely.geometry import Point\n",
    "#import arcpy\n",
    "#from arcgis.features import GeoAccessor\n",
    "\n",
    "\n",
    "#workspace =r'C:\\Users\\snewsome\\Documents\\ArcGIS\\Projects\\Data Management 2023\\Scratch.gdb'\n",
    "\n",
    "## Update SDE.Monitoring.SDE.Plan_Area_Noise with new data 2020-2023\n",
    "#excel_file_path = r\"F:\\Research and Analysis\\Noise\\Monitoring\\CNEL - Plan Areas\\ALL_CNEL_PLANAREAS_RESULTS_(1991-2023).xlsx\"\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "#Noise23_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Keep only columns that you want\n",
    "#columns_to_keep = ['CNEL LAND USE', '1987 PAS#', 'MONITORING SITE LATITUDE', 'MONITORING SITE LONGITUDE', 'PAS_NAME', 'LAND_USE', '2023 CNEL Maximum Day', '2022 CNEL Maximum Day', '2021 CNEL Maximum Day', '2020 CNEL Maximum Day']\n",
    "#Noise23_df = Noise23_df[columns_to_keep]\n",
    "\n",
    "#Noise23_df.columns = Noise23_df.columns.str.strip()\n",
    "\n",
    "# PASNAME\n",
    "#Noise23_df['PAS_NAME'] = Noise23_df['PAS_NAME'].astype(str)\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "#meltcolumns_to_keep = ['CNEL LAND USE', '1987 PAS#', 'MONITORING SITE LATITUDE', 'MONITORING SITE LONGITUDE', 'LAND_USE']\n",
    "#id_columns = ['PAS_NAME']\n",
    "\n",
    "# Specify the columns to melt (exclude the identifier column)\n",
    "#columns_to_melt = ['2023 CNEL Maximum Day', '2022 CNEL Maximum Day', '2021 CNEL Maximum Day', '2020 CNEL Maximum Day']\n",
    "\n",
    "#id_vars = id_columns + meltcolumns_to_keep\n",
    "\n",
    "# Use pd.melt to reshape the DataFrame\n",
    "#Noise23_df = pd.melt(Noise23_df, id_vars=id_vars, value_vars=columns_to_melt, var_name='Year', value_name='CNEL_Maximum_Day_Value')\n",
    "\n",
    "#print(Noise23_df.head())\n",
    "#print(Noise23_df.columns)\n",
    "\n",
    "# Extract only the year from the 'YEAR_OF_COUNT' column and drop \"CNEL Maximum Day\" from each value\n",
    "#Noise23_df['Year'] = Noise23_df['Year'].str.replace(' CNEL Maximum Day', '')\n",
    "\n",
    "\n",
    "\n",
    "# Add 'Category' column\n",
    "#Noise23_df['Category'] = 'CNEL Maximum Day'\n",
    "\n",
    "# Drop rows where 'CNEL_Maximum_Day_Value' is NaN\n",
    "#Noise23_df = Noise23_df.dropna(subset=['CNEL_Maximum_Day_Value'])\n",
    "\n",
    "# Use lookup_dict to fill in LTINFO# Create a lookup dictionary to fill in LTINFO and ID\n",
    "#gis = GIS()\n",
    "\n",
    "\n",
    "#def get_fs_data(service_url):\n",
    " #   feature_layer = FeatureLayer(service_url)\n",
    "  #  query_result = feature_layer.query()\n",
    "   # feature_list = query_result.features\n",
    "    #all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    #return all_data\n",
    "\n",
    "\n",
    "# REST SERVICE data for lookup dictionary\n",
    "#service_url = 'https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/6'\n",
    "\n",
    "#dfplannoisesde = get_fs_data(service_url)\n",
    "\n",
    "#dfplannoisesde.info()\n",
    "#columnstokeep = ['SITE_NAME', 'ID', 'LTINFO']\n",
    "#dfplannoisesde = dfplannoisesde.loc[:, columnstokeep]\n",
    "\n",
    "# Drop duplicates based on 'SITE_NAME' and keep the first occurrence\n",
    "#unique_values = dfplannoisesde.drop_duplicates(subset='SITE_NAME', keep='first')\n",
    "\n",
    "# Select specific columns for lookup\n",
    "#selected_columns = ['SITE_NAME', 'ID', 'LTINFO']\n",
    "\n",
    "# Convert selected columns to dictionary\n",
    "#lookup_dict = unique_values[selected_columns].set_index('SITE_NAME').to_dict(orient='index')\n",
    "\n",
    "# Display the dictionary\n",
    "#print(lookup_dict)\n",
    "\n",
    "# Create new columns 'ID' and 'LTINFO' based on 'PAS_NAME' and look up dictionary\n",
    "#Noise23_df['ID'] = Noise23_df['PAS_NAME'].apply(lambda x: lookup_dict.get(x, {}).get('ID'))\n",
    "#Noise23_df['LTINFO'] = Noise23_df['PAS_NAME'].apply(lambda x: lookup_dict.get(x, {}).get('LTINFO'))\n",
    "\n",
    "# Specify the columns to keep in Final_sdf\n",
    "#finalcolumns_to_keep = ['PAS_NAME', 'Year', 'CNEL_Maximum_Day_Value', 'Category','ID', 'MONITORING SITE LATITUDE', 'MONITORING SITE LONGITUDE', 'LAND_USE', 'CNEL LAND USE', 'LTINFO', '1987 PAS#']\n",
    "\n",
    "# Create a new DataFrame Final_df with the selected columns\n",
    "#Final_sdf = Noise23_df[finalcolumns_to_keep]\n",
    "\n",
    "# Create a GeoDataFrame by converting 'LATITUDE' and 'LONGITUDE' to Point geometry\n",
    "#geometry = [Point(xy) for xy in zip(Final_sdf['MONITORING SITE LONGITUDE'], Final_sdf['MONITORING SITE LATITUDE'])]\n",
    "#Final_sdf = gpd.GeoDataFrame(Final_sdf, geometry=geometry)\n",
    "\n",
    "# Field mapping so that columns match\n",
    "#field_mapping = {\n",
    " #   '1987 PAS#': 'PAS_1987',\n",
    "  #  'ID': 'ID',\n",
    "   # 'MONITORING SITE LATITUDE': 'LATITUDE',\n",
    "    #'MONITORING SITE LONGITUDE': 'LONGITUDE',\n",
    "    #'PAS_NAME': 'SITE_NAME',\n",
    "    #'LAND_USE': 'LAND_USE',\n",
    "    #'CNEL LAND USE': 'CNEL_LAND_USE',\n",
    "    #'Category': 'CATEGORY',\n",
    "    #'Year': 'YEAR_OF_COUNT',\n",
    "    #'CNEL_Maximum_Day_Value': 'COUNT_VALUE',\n",
    "    #'LTINFO': 'LTINFO'\n",
    "#}\n",
    "\n",
    "# Create a new DataFrame Final_df with columns based on the field mapping\n",
    "#Final_sdf = Final_sdf.rename(columns=field_mapping)\n",
    "\n",
    "# Convert GeoPandas DataFrame to ArcGIS Spatially Enabled DataFrame\n",
    "#sedf = GeoAccessor.from_geodataframe(Final_sdf)\n",
    "\n",
    "# Display the updated Final_df\n",
    "#print(list(Final_sdf.columns))\n",
    "\n",
    "# Save Final_df to a CSV file\n",
    "#Final_sdf.to_csv(r'F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Drafts\\NoiseThreshold23.csv', index=False)\n",
    "\n",
    "# Convert the SEDF to a feature class\n",
    "#sedf.spatial.to_featureclass(location=os.path.join(workspace, 'PlanAreaNoise23staging'), \n",
    " #                                                           sanitize_columns=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNEL Average Update SDE.Tabular SDE.Thresholdevaluation_PlanAreaNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update SDE.Tabular.sde.thresholdevaluation_PlanAreaNoise---Average CNEL over timeframe\n",
    "#updated to make a feature class instead of a csv\n",
    "# Setup\n",
    "import pandas as pd\n",
    "from arcgis import GIS\n",
    "from arcgis.features import FeatureLayer\n",
    "from arcgis.features import GeoAccessor\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "workspace =r'F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\n",
    "\n",
    "# Update SDE.tabular.SDE.threshhold_PlanAreaNoise with new data 2020-2023\n",
    "excel_file_path = r\"F:\\Research and Analysis\\Noise\\Monitoring\\CNEL - Plan Areas\\ALL_CNEL_PLANAREAS_RESULTS_(1991-2023).xlsx\"\n",
    "\n",
    "#update this Excel \n",
    "Excel_Final = r\"F:\\Research and Analysis\\Threshold reporting\\ThresholdData\\Noise\\ThresholdData_PlanAreaNoise.xlsx\"\n",
    "# Read the Excel file into a DataFrame\n",
    "Noise23_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Keep only columns that you want\n",
    "columns_to_keep = ['PAS_NAME', 'CNEL LAND USE', '2023 CNEL Average', '2022 CNEL Average', '2021 CNEL Average', '2020 CNEL Average', 'CNEL limit']\n",
    "Noise23_df = Noise23_df[columns_to_keep]\n",
    "\n",
    "Noise23_df.columns = Noise23_df.columns.str.strip()\n",
    "\n",
    "\n",
    "\n",
    "# melted columns to keep\n",
    "meltcolumns_to_keep = ['CNEL LAND USE']\n",
    "id_columns = ['PAS_NAME']\n",
    "\n",
    "# Specify the columns to melt (exclude the identifier column)\n",
    "columns_to_melt = ['2023 CNEL Average', '2022 CNEL Average', '2021 CNEL Average', '2020 CNEL Average']\n",
    "\n",
    "id_vars = id_columns + meltcolumns_to_keep\n",
    "\n",
    "# Use pd.melt to reshape the DataFrame\n",
    "Noise23_df = pd.melt(Noise23_df, id_vars=id_vars, value_vars=columns_to_melt, var_name='Year', value_name='CNEL_Average')\n",
    "\n",
    "# Extract only the year from the 'YEAR_OF_COUNT' column and drop \"CNEL Maximum Day\" from each value\n",
    "#Noise23_df['Year'] = Noise23_df['Year'].str.replace(' CNEL Average', '')\n",
    "# Extract only the year from the 'Year' column and drop \"CNEL Average\" from each value\n",
    "Noise23_df['Year'] = Noise23_df['Year'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# Drop rows where 'CNEL_Average' is NaN\n",
    "Noise23_df = Noise23_df.dropna(subset=['CNEL_Average'])\n",
    "\n",
    "print(Noise23_df.head())\n",
    "print(Noise23_df.columns)\n",
    "\n",
    "\n",
    "# Calculate the average for each category\n",
    "Value = Noise23_df.groupby('CNEL LAND USE')['CNEL_Average'].agg(np.mean)\n",
    "\n",
    "# Create new DataFrame with category averages\n",
    "Noiseavg_df = pd.DataFrame(Value).reset_index()\n",
    "Noiseavg_df.columns = ['CNEL LAND USE', 'Value']\n",
    "\n",
    "# Create new rows with the calculated averages and the desired year range\n",
    "year_range = '2020-2023'  # Define the desired year range\n",
    "\n",
    "Noiseavg_df['Year'] = '2020-2023'\n",
    "Noiseavg_df['Description'] = 'Average CNEL of all sites during timeframe'\n",
    "\n",
    "#Defining Threshold Value by category\n",
    "def categorize_value(row):\n",
    "    if row['CNEL LAND USE'] == 'COMMERCIAL':\n",
    "        return '60'\n",
    "    elif row['CNEL LAND USE'] == 'HIGH DENSITY RESIDENTIAL':\n",
    "        return '55'\n",
    "    elif row['CNEL LAND USE'] == 'HOTEL/MOTEL':\n",
    "        return '60'\n",
    "    elif row['CNEL LAND USE'] == 'INDUSTRIAL':\n",
    "        return '65'\n",
    "    elif row['CNEL LAND USE'] == 'LOW DENSITY RESIDENTIAL':\n",
    "        return '50'\n",
    "    elif row['CNEL LAND USE'] == 'URBAN OUTDOOR RECREATION':\n",
    "        return '55'\n",
    "    elif row['CNEL LAND USE'] == 'RURAL OUTDOOR RECREATION':\n",
    "        return '50'\n",
    "    elif row['CNEL LAND USE'] == 'WILDERNESS/ROADLESS':\n",
    "        return '45'\n",
    "\n",
    "    else:\n",
    "        return '45'\n",
    "\n",
    "# Apply the function to each row and assign the result to a new column 'Category_Type'\n",
    "Noiseavg_df['Threshold_Value'] = Noiseavg_df.apply(categorize_value, axis=1)\n",
    "\n",
    "#Defining Threshold Value by category\n",
    "def categorize_value(row):\n",
    "    if row['CNEL LAND USE'] == 'COMMERCIAL':\n",
    "        return 'Commercial Areas'\n",
    "    elif row['CNEL LAND USE'] == 'HIGH DENSITY RESIDENTIAL':\n",
    "        return 'High Density Residential'\n",
    "    elif row['CNEL LAND USE'] == 'HOTEL/MOTEL':\n",
    "        return 'Hotel / Motel Areas'\n",
    "    elif row['CNEL LAND USE'] == 'INDUSTRIAL':\n",
    "        return 'Industrial Areas'\n",
    "    elif row['CNEL LAND USE'] == 'LOW DENSITY RESIDENTIAL':\n",
    "        return 'Low Density Residential'\n",
    "    elif row['CNEL LAND USE'] == 'URBAN OUTDOOR RECREATION':\n",
    "        return 'Urban Outdoor Recreation Areas'\n",
    "    elif row['CNEL LAND USE'] == 'RURAL OUTDOOR RECREATION':\n",
    "        return 'Rural Outdoor Recreation Areas'\n",
    "    elif row['CNEL LAND USE'] == 'WILDERNESS/ROADLESS':\n",
    "        return 'Wilderness and Roadless'\n",
    "\n",
    "    else:\n",
    "        return 'Critical Wildlife Habitat'\n",
    "\n",
    "# Apply the function to each row and assign the result to a new column 'Category_Type'\n",
    "Noiseavg_df['Category'] = Noiseavg_df.apply(categorize_value, axis=1)\n",
    "\n",
    "print(Noiseavg_df)\n",
    "\n",
    "\n",
    "# Specify the columns to keep in Final_sdf\n",
    "finalcolumns_to_keep = [ 'Year', 'Threshold_Value','Description', 'Category', 'Value']\n",
    "\n",
    "# Create a new DataFrame Final_df with the selected columns\n",
    "df = Noiseavg_df[finalcolumns_to_keep]\n",
    "\n",
    "\n",
    "\n",
    "# Field mapping so that columns match\n",
    "field_mapping = {\n",
    "    'Threshold_Value': 'Threshold_Value',\n",
    "    'Category': 'Category',\n",
    "    'Description': 'Description',\n",
    "    'Year': 'Year',\n",
    "    'CNEL_Average': 'Value',\n",
    "    \n",
    "}\n",
    "\n",
    "# Create a new DataFrame Final_df with columns based on the field mapping\n",
    "df = df.rename(columns=field_mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update SDE.Monitoring.sde.Plan_Area_Noise for Monitoring Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updated to make a feature class instead of a csv\n",
    "# Setup\n",
    "import pandas as pd\n",
    "from arcgis import GIS\n",
    "from arcgis.features import FeatureLayer\n",
    "from arcgis.features import GeoAccessor\n",
    "import os\n",
    "\n",
    "\n",
    "workspace =r'F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\n",
    "\n",
    "# Update SDE.Monitoring.SDE.Plan_Area_Noise with new data 2020-2023\n",
    "excel_file_path = r\"F:\\Research and Analysis\\Noise\\Monitoring\\CNEL - Plan Areas\\ALL_CNEL_PLANAREAS_RESULTS_(1991-2023).xlsx\"\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "Noise23_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Keep only columns that you want\n",
    "columns_to_keep = ['CNEL LAND USE', 'MONITORING SITE LATITUDE', 'MONITORING SITE LONGITUDE', 'PAS_NAME', 'LAND_USE', '2023 CNEL Maximum Day', '2022 CNEL Maximum Day', '2021 CNEL Maximum Day', '2020 CNEL Maximum Day']\n",
    "Noise23_df = Noise23_df[columns_to_keep]\n",
    "\n",
    "Noise23_df.columns = Noise23_df.columns.str.strip()\n",
    "\n",
    "# PASNAME\n",
    "Noise23_df['PAS_NAME'] = Noise23_df['PAS_NAME'].astype(str)\n",
    "\n",
    "# melted columns to keep\n",
    "meltcolumns_to_keep = ['CNEL LAND USE', 'MONITORING SITE LATITUDE', 'MONITORING SITE LONGITUDE', 'LAND_USE']\n",
    "id_columns = ['PAS_NAME']\n",
    "\n",
    "# Specify the columns to melt (exclude the identifier column)\n",
    "columns_to_melt = ['2023 CNEL Maximum Day', '2022 CNEL Maximum Day', '2021 CNEL Maximum Day', '2020 CNEL Maximum Day']\n",
    "\n",
    "id_vars = id_columns + meltcolumns_to_keep\n",
    "\n",
    "# Use pd.melt to reshape the DataFrame\n",
    "Noise23_df = pd.melt(Noise23_df, id_vars=id_vars, value_vars=columns_to_melt, var_name='Year', value_name='CNEL_Maximum_Day_Value')\n",
    "\n",
    "print(Noise23_df.head())\n",
    "print(Noise23_df.columns)\n",
    "\n",
    "# Extract only the year from the 'YEAR_OF_COUNT' column and drop \"CNEL Maximum Day\" from each value\n",
    "Noise23_df['Year'] = Noise23_df['Year'].str.replace(' CNEL Maximum Day', '')\n",
    "\n",
    "\n",
    "\n",
    "# Add 'Category' column\n",
    "Noise23_df['Category'] = 'CNEL Maximum Day'\n",
    "\n",
    "# Drop rows where 'CNEL_Maximum_Day_Value' is NaN\n",
    "Noise23_df = Noise23_df.dropna(subset=['CNEL_Maximum_Day_Value'])\n",
    "\n",
    "# Use lookup_dict to fill in LTINFO# Create a lookup dictionary to fill in LTINFO and ID\n",
    "gis = GIS()\n",
    "\n",
    "\n",
    "def get_fs_data(service_url):\n",
    "    feature_layer = FeatureLayer(service_url)\n",
    "    query_result = feature_layer.query()\n",
    "    feature_list = query_result.features\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    return all_data\n",
    "\n",
    "\n",
    "# REST SERVICE data for lookup dictionary\n",
    "service_url = 'https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/6'\n",
    "\n",
    "dfplannoisesde = get_fs_data(service_url)\n",
    "\n",
    "dfplannoisesde.info()\n",
    "columnstokeep = ['SITE_NAME', 'ID', 'LTINFO', 'PAS_1987', 'LATITUDE', 'LONGITUDE']\n",
    "dfplannoisesde = dfplannoisesde.loc[:, columnstokeep]\n",
    "\n",
    "# Drop duplicates based on 'SITE_NAME' and keep the first occurrence\n",
    "unique_values = dfplannoisesde.drop_duplicates(subset='SITE_NAME', keep='first')\n",
    "\n",
    "# Select specific columns for lookup\n",
    "selected_columns = ['SITE_NAME', 'ID', 'LTINFO', 'PAS_1987', 'LATITUDE', 'LONGITUDE']\n",
    "\n",
    "def update_lat_lon_from_lookup_dict(noise_data, lookup_dict):\n",
    "    # Assuming 'SITE_NAME' is the common identifier between DataFrame and lookup dictionary\n",
    "    for index, row in noise_data.iterrows():\n",
    "        # Get the 'LATITUDE' and 'LONGITUDE' from the lookup dictionary using the 'SITE_NAME' from the DataFrame\n",
    "        lat_lon = lookup_dict.get(row['PAS_NAME'])\n",
    "        # Update the 'MONITORING SITE LATITUDE' and 'MONITORING SITE LONGITUDE' in the DataFrame with the 'LATITUDE' and 'LONGITUDE' from the lookup dictionary\n",
    "        if lat_lon is not None:\n",
    "            noise_data.at[index, 'MONITORING SITE LATITUDE'] = lat_lon['LATITUDE']\n",
    "            noise_data.at[index, 'MONITORING SITE LONGITUDE'] = lat_lon['LONGITUDE']\n",
    "\n",
    "    \n",
    "    return noise_data\n",
    "\n",
    "# Convert selected columns to dictionary\n",
    "lookup_dict = unique_values[selected_columns].set_index('SITE_NAME').to_dict(orient='index')\n",
    "\n",
    "# Display the dictionary\n",
    "print(lookup_dict)\n",
    "\n",
    "# Update 'MONITORING SITE LATITUDE' and 'MONITORING SITE LONGITUDE' in Noise23_df with data from the lookup dictionary\n",
    "Noise23_df = update_lat_lon_from_lookup_dict(Noise23_df, lookup_dict)\n",
    "\n",
    "\n",
    "\n",
    "# Create new columns 'ID' and 'LTINFO' based on 'PAS_NAME' and look up dictionary\n",
    "Noise23_df['ID'] = Noise23_df['PAS_NAME'].apply(lambda x: lookup_dict.get(x, {}).get('ID'))\n",
    "Noise23_df['LTINFO'] = Noise23_df['PAS_NAME'].apply(lambda x: lookup_dict.get(x, {}).get('LTINFO'))\n",
    "Noise23_df['PAS_1987'] = Noise23_df['PAS_NAME'].apply(lambda x: lookup_dict.get(x, {}).get('PAS_1987'))\n",
    "\n",
    "# Specify the columns to keep in Final_sdf\n",
    "finalcolumns_to_keep = ['PAS_NAME', 'Year', 'CNEL_Maximum_Day_Value', 'Category','ID', 'MONITORING SITE LATITUDE', 'MONITORING SITE LONGITUDE', 'LAND_USE', 'CNEL LAND USE', 'LTINFO', 'PAS_1987']\n",
    "\n",
    "# Create a new DataFrame Final_df with the selected columns\n",
    "df = Noise23_df[finalcolumns_to_keep]\n",
    "\n",
    "# Field mapping so that columns match\n",
    "field_mapping = {\n",
    "    'PAS_1987': 'PAS_1987',\n",
    "    'ID': 'ID',\n",
    "    'MONITORING SITE LATITUDE': 'LATITUDE',\n",
    "    'MONITORING SITE LONGITUDE': 'LONGITUDE',\n",
    "    'PAS_NAME': 'SITE_NAME',\n",
    "    'LAND_USE': 'LAND_USE',\n",
    "    'CNEL LAND USE': 'CNEL_LAND_USE',\n",
    "    'Category': 'CATEGORY',\n",
    "    'Year': 'YEAR_OF_COUNT',\n",
    "    'CNEL_Maximum_Day_Value': 'COUNT_VALUE',\n",
    "    'LTINFO': 'LTINFO'\n",
    "}\n",
    "\n",
    "# Create a new DataFrame Final_df with columns based on the field mapping\n",
    "df = df.rename(columns=field_mapping)\n",
    "\n",
    "# Convert DataFrame to Spatially Enabled DataFrame\n",
    "sedf = GeoAccessor.from_xy(df, x_column='LONGITUDE', y_column='LATITUDE')\n",
    "\n",
    "# Convert the SEDF to a feature class without sanitizing columns\n",
    "sedf.spatial.to_featureclass(location=os.path.join(workspace, 'PlanAreaNoise_23_Staging'), sanitize_columns=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reproject feature class 4236 to 26910\n",
    "# Set the input and output paths\n",
    "arcpy.env.workspace=r'F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\n",
    "input_feature_class = 'PlanAreaNoise_23_Staging'\n",
    "output_feature_class = 'PlanAreaNoise_23_ready'\n",
    " \n",
    "# Specify the target coordinate system (EPSG code 26910 for UTM Zone 10N)\n",
    "target_coordinate_system = arcpy.SpatialReference(26910)\n",
    " \n",
    "# Use the Project tool to reproject the feature class\n",
    "arcpy.management.Project(\n",
    "    input_feature_class,\n",
    "    output_feature_class,\n",
    "    target_coordinate_system\n",
    ")\n",
    " \n",
    "print(f\"Reprojection completed. Output feature class: {output_feature_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.Delete_management(r'c:\\Users\\snewsome\\Documents\\GitHub\\ThresholdEvaluation\\Scripts\\2023\\db_connect\\ConnectionFile.sde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "from time import strftime\n",
    "\n",
    "\n",
    "#push data to sde\n",
    "#Create database connection\n",
    "inWorkspace = r\"F:\\Research and Analysis\\Workspace\\Sarah\\SarahVector.sde\"\n",
    "arcpy.env.workspace = inWorkspace\n",
    "\n",
    "# Specify the name of the new version and the parent version\n",
    "new_version_name = \"PlanAreaNoise_\" + strftime(\"%Y-%m-%d\")\n",
    "parent_version = \"sde.DEFAULT\"\n",
    "version_name_full = '\"TAHOE\\SNEWSOME\".'+ new_version_name\n",
    "\n",
    "# List all versions in the geodatabase\n",
    "existing_versions = [version.name for version in arcpy.da.ListVersions(inWorkspace)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "from time import strftime\n",
    "\n",
    "\n",
    "#push data to sde\n",
    "#Create database connection\n",
    "inWorkspace=r\"F:\\Research and Analysis\\Workspace\\Sarah\\SarahVector.sde\"\n",
    "arcpy.env.workspace=inWorkspace\n",
    "\n",
    "# Specify the name of the new version and the parent version\n",
    "new_version_name = \"PlanAreaNoise_\" + strftime(\"%Y-%m-%d\")\n",
    "parent_version = \"sde.DEFAULT\"\n",
    "version_name_full = '\"TAHOE\\SNEWSOME\".'+ new_version_name\n",
    "\n",
    "# List all versions in the geodatabase\n",
    "existing_versions = [version.name for version in arcpy.da.ListVersions(inWorkspace)]\n",
    "\n",
    "# Check if the specified version exists\n",
    "if version_name_full in existing_versions:\n",
    "    # Delete the version\n",
    "    arcpy.management.DeleteVersion(inWorkspace, version_name_full)\n",
    "    print(f\"Version '{version_name_full}' deleted successfully.\")\n",
    "else:\n",
    "    print(f\"Version '{version_name_full}' does not exist.\")\n",
    "\n",
    "# Create a new version\n",
    "arcpy.CreateVersion_management(inWorkspace, parent_version, new_version_name, \"PUBLIC\")\n",
    "\n",
    "# If you want to create a connection file, you can use the following code:\n",
    "arcpy.CreateDatabaseConnection_management(\n",
    "    out_folder_path='db_connect/',\n",
    "    out_name=\"ConnectionFile.sde\",\n",
    "    database_platform=\"SQL_SERVER\",  \n",
    "    instance='sql12',\n",
    "    database='sde',\n",
    "    account_authentication=\"OPERATING_SYSTEM_AUTH\",  \n",
    "    version_type='TRANSACTIONAL',\n",
    "    version=version_name_full\n",
    ")\n",
    "\n",
    "PlanNoisesde=os.path.join(version_name_full, \"SDE.Monitoring\\SDE.Plan_Area_Noise\")\n",
    "PlanAreaNoise = 'planareanoise'\n",
    "arcpy.MakeFeatureLayer_management(PlanNoisesde, PlanAreaNoise)\n",
    "\n",
    "\n",
    "# Create a new version\n",
    "#arcpy.CreateVersion_management(inWorkspace, parent_version, new_version_name, \"PUBLIC\")\n",
    "arcpy.ChangeVersion_management(PlanAreaNoise, 'TRANSACTIONAL', version_name_full, '')\n",
    "\n",
    "#Start\n",
    "edit = arcpy.da.Editor('db_connections/ConnectionFile.sde')\n",
    "edit.startEditing(False, True)\n",
    "\n",
    "# Append the records from the temporary feature class to the target feature class\n",
    "arcpy.management.Append(\n",
    "    inputs='PlanAreaNoise_23_ready',\n",
    "    target=PlanNoisesde, \n",
    "    schema_type='NO_TEST'\n",
    ")\n",
    "\n",
    "#Stop\n",
    "edit.stopEditing(True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    arcpy.CreateVersion_management(inWorkspace, parent_version, new_version_name, \"PUBLIC\")\n",
    "    print(\"Version created successfully.\")\n",
    "except arcpy.ExecuteError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start\n",
    "edit = arcpy.da.Editor('db_connections/ConnectionFile.sde')\n",
    "edit.startEditing(False, True)\n",
    "\n",
    "# Append the records from the temporary feature class to the target feature class\n",
    "arcpy.management.Append(\n",
    "    inputs='PlanAreaNoise_23_ready',\n",
    "    target=os.path.join(version_full_name, \"SDE.Monitoring\\SDE.Plan_Area_Noise\"), \n",
    "    schema_type='NO_TEST'\n",
    ")\n",
    "\n",
    "#Stop\n",
    "edit.stopEditing(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.Delete_management(\"planareanoise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start an edit session\n",
    "edit = arcpy.da.Editor('db_connections/ConnectionFile.sde')\n",
    "edit.startEditing(False, True)\n",
    "\n",
    "    # Create a new version\n",
    "arcpy.CreateVersion_management(inWorkspace, parent_version, new_version_name, \"PUBLIC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shorezone Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream/CSCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the Average CSCI score for TPAandTPBsites for 2020-2022 and fill in csv, Next code block will be splitting TPA and TPB into two different avg scores\n",
    "#Setup\n",
    "#Create Dictionary Usring Rest Service data\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from arcgis import GIS\n",
    "from arcgis.features import FeatureLayer\n",
    "import numpy as np\n",
    "import requests\n",
    "import arcpy\n",
    "\n",
    "gis = GIS()\n",
    "# Connect to TRPA Enterprise GIS Portal *if it's a service only shared with org\n",
    "# portal_user = \"TRPA_PORTAL_ADMIN\"\n",
    "# portal_pwd = str(os.environ.get('Password'))\n",
    "# portal_url = \"https://maps.trpa.org/portal/\"\n",
    "wk_memory = \"memory\" + \"\\\\\"\n",
    "\n",
    "#Set up Workspace\n",
    "# set workspace and sde connections \n",
    "working_folder = r\"F:\\Research and Analysis\\Threshold reporting\\ThresholdData\\Fisheries\"\n",
    "workspace      = \"C:\\GIS\\Scratch.gdb\"\n",
    "arcpy.env.workspace = \"C:\\GIS\\Scratch.gdb\"\n",
    "\n",
    "# network path to connection files\n",
    "filePath = r\"C:\\\\GIS\\\\DB_CONNECT\"\n",
    "\n",
    "\n",
    "## CSV to be updated with new average\n",
    "StreamAvg = os.path.join(working_folder,\"ThresholdData_Stream.csv\")\n",
    "\n",
    "\n",
    "def get_fs_data(service_url):\n",
    "    feature_layer = FeatureLayer(service_url)\n",
    "    query_result = feature_layer.query()\n",
    "    # Convert the query result to a list of dictionaries\n",
    "    feature_list = query_result.features\n",
    "    # Create a pandas DataFrame from the list of dictionaries\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    return all_data\n",
    "\n",
    "# Service URL\n",
    "service_url = 'https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/8'\n",
    "\n",
    "# Get Stream data as a Spatially Enabled DataFrame\n",
    "sdfStreamHab = get_fs_data(service_url)\n",
    "\n",
    "# Keep only necessary columns\n",
    "columnstokeep = ['SITE_NAME', 'STATION_TYPE', 'DURATION', 'LATITUDE', 'LONGITUDE', 'LTINFO', 'COUNT_VALUE', 'YEAR_OF_COUNT']\n",
    "sdfStreamHab = sdfStreamHab.loc[:, columnstokeep]\n",
    "\n",
    "# Query only 2020-2022 data\n",
    "sdfStreamHab['Date'] = pd.to_datetime(sdfStreamHab['YEAR_OF_COUNT'], format='%Y')\n",
    "filtered_df = sdfStreamHab[\n",
    "    (sdfStreamHab['Date'].dt.year.between(2020, 2022)) & \n",
    "    (sdfStreamHab['SITE_NAME'].str.contains('TPB|TPA'))\n",
    "].copy()\n",
    "\n",
    "# Calculate the average of the 'COUNT_VALUE' column\n",
    "AvgCSCI23 = filtered_df['COUNT_VALUE'].mean()\n",
    "\n",
    "# Update CSV ThresholdData_Stream.csv\n",
    "csv_path = r\"F:\\Research and Analysis\\Threshold reporting\\ThresholdData\\Fisheries\\ThresholdData_Streams.csv\"\n",
    "existing_df = pd.read_csv(csv_path)\n",
    "\n",
    "Description = \"Average CSCI score of all trend sites (48 sites)\"\n",
    "Year = '2020-2022'\n",
    "Value = AvgCSCI\n",
    "\n",
    "existing_entry = existing_df[\n",
    "    (existing_df['Description'] == Description) &\n",
    "    (existing_df['Year'] == Year)\n",
    "]\n",
    "\n",
    "if existing_entry.empty:\n",
    "    new_entry = {'Description': Description, 'Year': Year, 'Value': Value}\n",
    "    existing_df = pd.concat([existing_df, pd.DataFrame([new_entry])], ignore_index=True)\n",
    "\n",
    "\n",
    "existing_df.to_csv(csv_path, index=False)\n",
    "print(f\"CSV file '{csv_path}' has been updated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avg Stream CSCI split into TPB and TPA for all years. in a new excel sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from arcgis import GIS\n",
    "from arcgis.features import FeatureLayer\n",
    "import numpy as np\n",
    "\n",
    "import arcpy\n",
    "\n",
    "gis = GIS()\n",
    "\n",
    "wk_memory = \"memory\" + \"\\\\\"\n",
    "\n",
    "#Set up Workspace\n",
    "# set workspace and sde connections \n",
    "working_folder = r\"F:\\Research and Analysis\\Threshold reporting\\ThresholdData\\Fisheries\"\n",
    "workspace      = \"F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb\"\n",
    "arcpy.env.workspace = \"F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb\"\n",
    "\n",
    "\n",
    "\n",
    "## excel to be updated with new averages split into TPA and TPB\n",
    "StreamAvg = os.path.join(working_folder,\"ThresholdData_StreamsTavg.xlsx\")\n",
    "\n",
    "\n",
    "def get_fs_data(service_url):\n",
    "    feature_layer = FeatureLayer(service_url)\n",
    "    query_result = feature_layer.query()\n",
    "    # Convert the query result to a list of dictionaries\n",
    "    feature_list = query_result.features\n",
    "    # Create a pandas DataFrame from the list of dictionaries\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    return all_data\n",
    "\n",
    "# Service URL\n",
    "service_url = 'https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/8'\n",
    "\n",
    "# Get Stream data as REST SERVICE to a Spatially Enabled DataFrame\n",
    "sdfStreamHab = get_fs_data(service_url)\n",
    "\n",
    "# Keep only necessary columns\n",
    "columnstokeep = ['SITE_NAME', 'STATION_TYPE', 'DURATION', 'LATITUDE', 'LONGITUDE', 'LTINFO', 'COUNT_VALUE', 'YEAR_OF_COUNT']\n",
    "sdfStreamHab = sdfStreamHab.loc[:, columnstokeep]\n",
    "\n",
    "#This is where the magic calculations happen for each threshold\n",
    "# Mke year of count into date format\n",
    "sdfStreamHab['Date'] = pd.to_datetime(sdfStreamHab['YEAR_OF_COUNT'], format='%Y')\n",
    "\n",
    "# Define bins for grouping by year ranges\n",
    "bins = [2010, 2012, 2014, 2016, 2020, 2023]\n",
    "\n",
    "# Define labels for the bins\n",
    "labels = ['2010-2011', '2012-2013', '2014-2015', '2016-2019', '2020-2022']\n",
    "\n",
    "# Create a new column 'YearRange' to group the data by year ranges\n",
    "sdfStreamHab['YearRange'] = pd.cut(sdfStreamHab['Date'].dt.year, bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Filter the DataFrame for TPB sites and TPA sites\n",
    "filtered_df_tpb = sdfStreamHab[sdfStreamHab['SITE_NAME'].str.contains('TPB')]\n",
    "filtered_df_tpa = sdfStreamHab[sdfStreamHab['SITE_NAME'].str.contains('TPA')]\n",
    "\n",
    "# Group by 'YearRange', then calculate the average 'COUNT_VALUE' TPB sites\n",
    "averages_tpb = filtered_df_tpb.groupby('YearRange')['COUNT_VALUE'].mean().reset_index()\n",
    "averages_tpb.rename(columns={'COUNT_VALUE': 'Average_COUNT_VALUE_TPB'}, inplace=True)\n",
    "\n",
    "averages_tpa = filtered_df_tpa.groupby('YearRange')['COUNT_VALUE'].mean().reset_index()\n",
    "averages_tpa.rename(columns={'COUNT_VALUE': 'Average_COUNT_VALUE_TPA'}, inplace=True)\n",
    "\n",
    "# Create Description and Year columns in both DataFrames\n",
    "averages_tpb['Description'] = 'Average CSCI Score per Trend Panel(24 Sites)'\n",
    "averages_tpa['Description'] = 'Average CSCI Score per Trend Panel(24 Sites)'\n",
    "averages_tpb['Year'] = averages_tpb['YearRange']\n",
    "averages_tpa['Year'] = averages_tpa['YearRange']\n",
    "\n",
    "# Merge the two DataFrames on 'Year' and 'Description' to create the final merged DataFrame\n",
    "FAdf = pd.merge(averages_tpb, averages_tpa, on=['Year', 'Description'], how='outer')\n",
    "# Sort by 'Year' for clarity\n",
    "FAdf.sort_values('Year', inplace=True)\n",
    "\n",
    "# Reset index for clarity\n",
    "FAdf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Define columns to keep\n",
    "columns_to_keep = ['Year', 'Description', 'Average_COUNT_VALUE_TPB', 'Average_COUNT_VALUE_TPA']\n",
    "\n",
    "# Select only the desired columns\n",
    "FAdf = FAdf[columns_to_keep]\n",
    "# Print the combined averages\n",
    "print(FAdf)\n",
    "print(\"FAdf:\")\n",
    "\n",
    "# Write the updated DataFrame back to the CSV file\n",
    "FAdf.to_excel(StreamAvg, index=False)\n",
    "\n",
    "print(f\"The excel file '{StreamAvg}' has been updated with new averages successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use F:Research and Analysis/Threshold reporting\\ThresholdData\\Fisheries\\ThresholdData_Streams.csv' to update SDE.tabular.....table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Rating or CSCI values and then find the percent of total streams in each rating\n",
    "excel_file_path=\"F:\\Research and Analysis\\Fisheries\\Streams\\Bioassessment\\California Stream Condition Index\\California Stream Condition Index\\CSCI_Scores_AllSites_AllYears.xlsx\"\n",
    "\n",
    "Alldf = pd.read_excel(excel_file_path, sheet_name='CSCI_Scores_AllSites_AllYears')\n",
    "\n",
    "#Calculate Rating for CSCI value\n",
    "#Define a function to categorize CSCI values based on ranges\n",
    "def categorize_value(value):\n",
    "    if 0 <= value < 0.8:\n",
    "        return 'marginal'\n",
    "    elif 0.8 <= value < 1.0:\n",
    "        return 'good'\n",
    "    else:\n",
    "        return 'excellent'\n",
    "\n",
    "Alldf['Rating']=Alldf['CSCI_Score'].apply(categorize_value)\n",
    "\n",
    "print(Alldf)\n",
    "\n",
    "#Find average for each rating- Poor, Marginal, Good, Excellent\n",
    "# Group by 'Category' and calculate the mean of 'Count'\n",
    "average_count_by_category = df.groupby('Rating')['CSCI_Score'].mean()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wild Life"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Wildlifedf['Wildlife_Species'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wildlife Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import pandas as pd\n",
    "import os\n",
    "# Delete the table\n",
    "\n",
    "workspace = r'F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\n",
    "#arcpy.Delete_management(os.path.join(workspace, 'Wildlife_Staging'))\n",
    "# Update SDE.tabular.SDE.threshhold_PlanAreaNoise with new data 2020-2023\n",
    "excel_file_path = r\"F:\\Research and Analysis\\Threshold reporting\\ThresholdData\\Wildlife\\Wildlife_ThresholdData.xlsx\"\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "Rawdf = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Grab needed data by year\n",
    "Wildlifedf = Rawdf.loc[(Rawdf['Year'] >= 2020) & (Rawdf['Year'] <= 2023)].copy()\n",
    "\n",
    "#Rename columns directly\n",
    "Wildlifedf.rename(columns={'Wildlife Species': 'Wildlife_Species', 'Threshold Value': 'Threshold_Value', 'Count': 'Total'}, inplace=True)\n",
    "# Convert 'Wildlife_Species' column to string type\n",
    "Wildlifedf['Wildlife_Species'] = Wildlifedf['Wildlife_Species'].astype(str)\n",
    "# Define the fields to include in the table\n",
    "field_names = ['Wildlife_Species', 'Threshold_Value', 'Category', 'Year', 'Total']  # Add more fields if necessary\n",
    "\n",
    "# Convert 'Wildlife Species' column to string type to ensure compatibility\n",
    "Wildlifedf['Wildlife_Species'] = Wildlifedf['Wildlife_Species'].astype(str)\n",
    "\n",
    "# Define the fields to include in the table with specified field types\n",
    "field_types = {'Wildlife_Species': 'Text', 'Threshold_Value': 'Text', 'Category': 'TEXT', 'Year': 'Double', 'Total': 'Double'}\n",
    "\n",
    "# Create an empty table in the workspace\n",
    "table_path = os.path.join(workspace, 'Wildlife_staging')\n",
    "arcpy.CreateTable_management(workspace, 'Wildlife_staging')\n",
    "\n",
    "# Add fields to the table with specified field types\n",
    "for field_name, field_type in field_types.items():\n",
    "    arcpy.AddField_management(table_path, field_name, field_type)\n",
    "\n",
    "# Convert the DataFrame to a structured numpy array\n",
    "array = Wildlifedf[list(field_types.keys())].to_records(index=False)\n",
    "\n",
    "# Insert data into the table\n",
    "with arcpy.da.InsertCursor(table_path, list(field_types.keys())) as cursor:\n",
    "    for row in array:\n",
    "        cursor.insertRow(row)\n",
    "\n",
    "\n",
    "        #GO TO WORKSPACE FOLDER AND APPEND TO F:\\GIS\\DB_CONNECT\\Tabular.sde\\SDE.ThresholdEvaluation_Wildlife in PRO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waterfowl Human Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not done\n",
    "import arcpy\n",
    "import pandas as pd\n",
    "import os\n",
    "# Delete the table\n",
    "\n",
    "workspace = r'F:\\Research and Analysis\\Workspace\\Sarah\\Data Management 2023\\Scratch.gdb'\n",
    "#arcpy.Delete_management(os.path.join(workspace, 'Wildlife_Staging'))\n",
    "# Update SDE.tabular.SDE.threshhold_PlanAreaNoise with new data 2020-2023\n",
    "excel_file_path = r\"F:\\Research and Analysis\\Threshold reporting\\ThresholdData\\Wildlife\\Waterfowl_HumanActivityRating_2019.xlsx\"\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "Rawdf = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Grab needed data by year\n",
    "Wildlifedf = Rawdf.loc[(Rawdf['Year'] >= 2020) & (Rawdf['Year'] <= 2023)].copy()\n",
    "\n",
    "#Rename columns directly\n",
    "fowldf.rename(columns={'Wildlife Species': 'Wildlife_Species', 'Threshold Value': 'Threshold_Value', 'Count': 'Total'}, inplace=True)\n",
    "# Convert 'Wildlife_Species' column to string type\n",
    "fowldf['Wildlife_Species'] = fowldf['Wildlife_Species'].astype(str)\n",
    "# Define the fields to include in the table\n",
    "field_names = ['Wildlife_Species', 'Threshold_Value', 'Category', 'Year', 'Total']  # Add more fields if necessary\n",
    "\n",
    "# Convert 'Wildlife Species' column to string type to ensure compatibility\n",
    "Wildlifedf['Wildlife_Species'] = Wildlifedf['Wildlife_Species'].astype(str)\n",
    "\n",
    "# Define the fields to include in the table with specified field types\n",
    "field_types = {'Wildlife_Species': 'Text', 'Threshold_Value': 'Text', 'Category': 'TEXT', 'Year': 'Double', 'Total': 'Double'}\n",
    "\n",
    "# Create an empty table in the workspace\n",
    "table_path = os.path.join(workspace, 'Wildlife_staging')\n",
    "arcpy.CreateTable_management(workspace, 'Wildlife_staging')\n",
    "\n",
    "# Add fields to the table with specified field types\n",
    "for field_name, field_type in field_types.items():\n",
    "    arcpy.AddField_management(table_path, field_name, field_type)\n",
    "\n",
    "# Convert the DataFrame to a structured numpy array\n",
    "array = Wildlifedf[list(field_types.keys())].to_records(index=False)\n",
    "\n",
    "# Insert data into the table\n",
    "with arcpy.da.InsertCursor(table_path, list(field_types.keys())) as cursor:\n",
    "    for row in array:\n",
    "        cursor.insertRow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wildlife Data confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REST Service that refers to ['sde_tabular.SDE.ThresholdEvaluation_Waterfowl']\n",
    "\n",
    "#['sde_tabular.SDE.ThresholdEvaluation_Wildlife']\n",
    "def get_fs_data(wildlife_url):\n",
    "    feature_layer = FeatureLayer(wildlife_url)\n",
    "    query_result = feature_layer.query()\n",
    "    feature_list = query_result.features\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    return all_data\n",
    "\n",
    "waterfowl_url = \"https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/94\"\n",
    "wildlife_url = \"https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/96\"\n",
    "draftworkspace = r\"C:\\Users\\snewsome\\Documents\\GitHub\\ThresholdEvaluation\\2023\\Wildlife\\Chart\\Draft\"\n",
    "dfWildlife = get_fs_data(wildlife_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wintering Bald Eagle linear regression confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope: 0.847179487179487\n",
      "95% Confidence Interval for Slope: [0.5454479896893918, 1.1489109846695822]\n",
      "R-squared: 0.5831917257885286\n",
      "P-value: 5.65430032472803e-06\n",
      "dof: 24\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import linregress, t\n",
    "\n",
    "# Setup dataframe\n",
    "df = dfWildlife.loc[dfWildlife['Wildlife_Species'] == 'Bald Eagle - winter']\n",
    "\n",
    "# Calculate linear regression using scipy's linregress\n",
    "slope, intercept, r_value, p_value, std_err = linregress(df['Year'], df['Total'])\n",
    "r_squared = r_value ** 2\n",
    "\n",
    "# Calculate the degrees of freedom\n",
    "n = len(df)\n",
    "dof = n - 2\n",
    "\n",
    "# Calculate two-tailed t-value for 95% confidence interval\n",
    "t_value = t.ppf(0.975, dof)  # 0.975 for a two-tailed 95% confidence level\n",
    "\n",
    "# Calculate confidence interval for the slope\n",
    "slope_ci_lower = slope - t_value * std_err\n",
    "slope_ci_upper = slope + t_value * std_err\n",
    "\n",
    "\n",
    "\n",
    "# Display results\n",
    "print(f\"Slope: {slope}\")\n",
    "print(f\"95% Confidence Interval for Slope: [{slope_ci_lower}, {slope_ci_upper}]\")\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "print(f\"dof: {dof}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Osprey 95 confidence interval of linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope: 0.26678876678876684\n",
      "95% Confidence Interval for Slope: [0.08086824630055772, 0.45270928727697596]\n",
      "R-squared: 0.2589110593305362\n",
      "P-value: 0.006720059967321667\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import linregress, t\n",
    "\n",
    "# setup dataframe\n",
    "df = dfWildlife.loc[dfWildlife['Wildlife_Species'] == 'Osprey']\n",
    "\n",
    "# Calculate linear regression using scipy's linregress\n",
    "slope, intercept, r_value, p_value, std_err = linregress(df['Year'], df['Total'])\n",
    "r_squared = r_value ** 2\n",
    "\n",
    "# Calculate the degrees of freedom\n",
    "n = len(df)\n",
    "dof = n - 2\n",
    "\n",
    "# Calculate two-tailed t-value for 95% confidence interval\n",
    "t_value = t.ppf(0.975, dof)  # 0.975 for a two-tailed 95% confidence level\n",
    "\n",
    "# Calculate confidence interval for the slope\n",
    "slope_ci_lower = slope - t_value * std_err\n",
    "slope_ci_upper = slope + t_value * std_err\n",
    "\n",
    "# Display results\n",
    "print(f\"Slope: {slope}\")\n",
    "print(f\"95% Confidence Interval for Slope: [{slope_ci_lower}, {slope_ci_upper}]\")\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Falcon 95 confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope: 0.31029411764705883\n",
      "95% Confidence Interval for Slope: [0.2240225956947095, 0.39656563959940816]\n",
      "R-squared: 0.8095463223929447\n",
      "P-value: 2.0861186437493537e-06\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import linregress, t\n",
    "\n",
    "# setup dataframe\n",
    "df = dfWildlife.loc[dfWildlife['Wildlife_Species'] == 'Peregrine Falcon']\n",
    "\n",
    "# Calculate linear regression using scipy's linregress\n",
    "slope, intercept, r_value, p_value, std_err = linregress(df['Year'], df['Total'])\n",
    "r_squared = r_value ** 2\n",
    "\n",
    "# Calculate the degrees of freedom\n",
    "n = len(df)\n",
    "dof = n - 2\n",
    "\n",
    "# Calculate two-tailed t-value for 95% confidence interval\n",
    "t_value = t.ppf(0.975, dof)  # 0.975 for a two-tailed 95% confidence level\n",
    "\n",
    "# Calculate confidence interval for the slope\n",
    "slope_ci_lower = slope - t_value * std_err\n",
    "slope_ci_upper = slope + t_value * std_err\n",
    "\n",
    "# Display results\n",
    "print(f\"Slope: {slope}\")\n",
    "print(f\"95% Confidence Interval for Slope: [{slope_ci_lower}, {slope_ci_upper}]\")\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watercraft Inspections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "url = 'https://www.laketahoeinfo.org/WebServices/GetReportedEIPIndicatorProjectAccomplishments/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476/16'\n",
    "\n",
    "df = pd.read_json(url)\n",
    "df\n",
    "\n",
    "# stacked bar chart with plotly\n",
    "fig = px.bar(df, x='IndicatorProjectYear', y='IndicatorProjectValue', color='PMSubcategoryOption1', barmode='stack')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Quality Data to table in Arc Pro to be appended manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------#\n",
    "#Add Data to SDE\n",
    "#------------------------#\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Specify File Paths\n",
    "file_paths = [\"F:\\Research and Analysis\\Air Quality\\Annual Reports DRI\\AQ data 2023.xlsx\",\n",
    "              \"F:\\Research and Analysis\\Air Quality\\Annual Reports DRI\\AQ data 2022.xlsx\",\n",
    "              \"F:\\Research and Analysis\\Air Quality\\Annual Reports DRI\\AQ data 2021.xlsx\",\n",
    "              \"F:\\Research and Analysis\\Air Quality\\Annual Reports DRI\\AQ data 2020.xlsx\"]\n",
    "\n",
    "# 2. Read Data from Each File\n",
    "dfs = []  # List to store DataFrames from each file\n",
    "sheet_name = 'Indicator Values'  # Name of the sheet to read\n",
    "\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    dfs.append(df)\n",
    "\n",
    "# 3. Concatenate DataFrames\n",
    "Airdata_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "#Create layer in arcgis pro to be appended to sde.ThresholdEvaluyation_AirQuality\n",
    "\n",
    "#Fix up field names\n",
    "\n",
    "# Rename fields\n",
    "Airdata_df.rename(columns={'Data?': 'Data', 'Data Source': 'Data_Source'}, inplace=True)\n",
    "#Add columsn to dataframe\n",
    "Airdata_df['Threshold_Value']= None\n",
    "Airdata_df['Percent_of_Threshold_Value']= None\n",
    "Airdata_df['Include_in_Trend_Analysis']= None\n",
    "print(Airdata_df)\n",
    "\n",
    "\n",
    "# Improve code next year , ADD DLBLISS as yes for pm2.5 high24, south lake tahoe sites for pm2.5 avg. etc.\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = Airdata_df.to_dict(orient='records')\n",
    "\n",
    "# Get the field names from the field mapping\n",
    "field_names = list(Airdata_df.columns)\n",
    "\n",
    "\n",
    "# Create an empty table in the workspace\n",
    "table_path = os.path.join(workspace, 'Air_staging2')\n",
    "arcpy.CreateTable_management(workspace, 'Air_staging2')\n",
    "\n",
    "# Define the fields to include in the table with specified field types\n",
    "field_types = {'Indicator': 'Text', 'Pollutant': 'Text', 'Statistic': 'Text', 'Year': 'Long', 'Data': 'Text', 'Site': 'Text', 'Value': 'Double', 'Data_Source': 'Text', 'Exceedances': 'Long', 'Threshold_Value': 'Text', 'Percent_of_Threshold_Value': 'Text', 'Include_in_Trend_Analysis': 'Text'}\n",
    "\n",
    "# Add fields to the table with specified field types\n",
    "for field_name, field_type in field_types.items():\n",
    "    arcpy.AddField_management(table_path, field_name, field_type)\n",
    "\n",
    "# Convert the DataFrame to a structured numpy array\n",
    "array = Airdata_df[list(field_types.keys())].to_records(index=False)\n",
    "\n",
    "# Insert data into the table\n",
    "with arcpy.da.InsertCursor(table_path, list(field_types.keys())) as cursor:\n",
    "    for row in array:\n",
    "        cursor.insertRow(row)\n",
    "\n",
    "print(\"Data table successfully made.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearshore Periphyton/Algae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set colors\n",
    "color_discrete_map = {'Incline West': '#33a02c',\n",
    "                      'Pineland': '#1f78b4', \n",
    "                      'Rubicon': '#fb9a99',\n",
    "                      'Sugarpine': '#911eb4',\n",
    "                      'Tahoe City': '#ff7f00',\n",
    "                      'Zephyr': '#cab2d6'                                \n",
    "                        }\n",
    "#Import DAta\n",
    "file_path = r\"F:\\Research and Analysis\\Water Quality Monitoring Program\\Nearshore\\IntegratedAlgaeMonitoring\\data\\Peri\"\n",
    "\n",
    "# Load each CSV file into a DataFrame\n",
    "inclinedf = pd.read_csv(os.path.join(file_path, 'InclineWest_Historic.csv'))\n",
    "Pinelanddf = pd.read_csv(os.path.join(file_path, 'Pineland_Historic.csv'))\n",
    "Rubicondf = pd.read_csv(os.path.join(file_path, 'Rubicon_Historic.csv'))\n",
    "Sugarpinedf = pd.read_csv(os.path.join(file_path, 'Sugarpine_Historic.csv'))\n",
    "TahoeCitydf = pd.read_csv(os.path.join(file_path, 'TahoeCity_Historic.csv'))\n",
    "Zephyrdf = pd.read_csv(os.path.join(file_path, 'Zephyr_Historic.csv'))\n",
    "\n",
    "#Combine All Dataframes\n",
    "combined_df = pd.concat([inclinedf, Pinelanddf, Rubicondf, Sugarpinedf, TahoeCitydf, Zephyrdf], ignore_index=True)\n",
    "# Ensure the Date column is in datetime format\n",
    "combined_df['Sample_Date'] = pd.to_datetime(combined_df['Sample_Date'])\n",
    "\n",
    "# Extract the year from the Date column and create a new Year column\n",
    "combined_df['Year'] = combined_df['Sample_Date'].dt.year\n",
    "\n",
    "#Group by 'Year' and 'Site', then calculate the average of the 'chl' column\n",
    "grouped_df = combined_df.groupby(['Year', 'site'], as_index=False)['Chl'].mean()\n",
    "\n",
    "# Preview the resulting DataFrame\n",
    "print(grouped_df.head())\n",
    "\n",
    "#May need to drop rows with missing value? not sure how it will show up on chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "color_discrete_map = {'Incline West': '#008080',\n",
    "                      'Pineland': '#FF6F61', \n",
    "                      'Rubicon Pt.': '#cab2d6',\n",
    "                      'Sugar Pine Pt.': '#4169E1',\n",
    "                      'Tahoe City': '#DAA520',\n",
    "                      'Zephyr Pt.': '#708090'                                \n",
    "                        }\n",
    "\n",
    "df = grouped_df\n",
    "\n",
    "\n",
    "# setup plot\n",
    "fig = px.line(df, x = 'Year', y= 'Chl', color='site',\n",
    "                 color_discrete_map = color_discrete_map)\n",
    "\n",
    "fig.update_traces(hovertemplate='<br>%{y:.2f}')\n",
    "\n",
    "\n",
    "# set layout\n",
    "fig.update_layout(title=\"Nearshore Attached Algae - Average Chlorophyll\",\n",
    "                    font_family=font,\n",
    "                    template=template,\n",
    "                    showlegend=True,\n",
    "                    hovermode=\"x unified\",\n",
    "                    xaxis = dict(\n",
    "                        tickmode = 'linear',\n",
    "                        tick0 = 1985,\n",
    "                        dtick = 5\n",
    "                    ),\n",
    "                    yaxis = dict(\n",
    "                        tickmode = 'linear',\n",
    "                        tick0 = 0,\n",
    "                        dtick = 5,\n",
    "                        range=[0, 110],\n",
    "                        title_text='Average Chl'\n",
    "                    )\n",
    "                  \n",
    "                 )\n",
    "\n",
    "# show figure\n",
    "fig.show()\n",
    "# save to HTML\n",
    "fig.write_html(os.path.join(workspace, \"WaterQuality_NearshoreChl.html\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
